{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45544a79",
   "metadata": {},
   "source": [
    "# STOPWORDS\n",
    "\n",
    "Stopwords are common words (e.g. \"is\", \"the\") that are removed in NLP because they do not contribute significantly to the meaning of the text. This preprocessing aims to eliminate irrelevant and auxiliary terms, focusing the analysis on the essential content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20155a8d",
   "metadata": {},
   "source": [
    "Removing stopwords simplifies sentences by eliminating common words like **\"I\", \"have\", \"been\", \"the\", \"and\", \"that\", \"are\", \"about\", \"of\", focusing on key terms such as \"studied\", \"effects\", \"global warming\", \"noticed\", \"studies\", \"inconclusive\", \"results\"**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d130bcaa",
   "metadata": {},
   "source": [
    "Stopword extraction (or removal) offers several key benefits in Natural Language Processing (NLP):\n",
    "\n",
    "1. **Dimensionality Reduction:** Reduces the total number of unique words (vocabulary) in a corpus, which optimizes memory usage and speeds up processing in ML models.\n",
    "2. **Focus on Relevant Words:** Allows NLP algorithms to focus on terms that actually carry meaning and contextual information, improving the relevance of the analysis.\n",
    "3. **Improved Efficiency and Performance:** By dealing with less data, algorithms run faster and more efficiently, which is crucial for large volumes of text.\n",
    "4. **Increased Accuracy in Specific Tasks:** In tasks such as text classification, sentiment analysis, and information retrieval, stopword removal can improve the accuracy of results, as irrelevant words do not \"pollute\" the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdd0cd1",
   "metadata": {},
   "source": [
    "For the practice of stopword removal, we will use NLTK's 20Newsgroups corpus. This dataset, consisting of about 20,000 articles from old online forums (newsgroups) divided into 20 categories, is widely used in machine learning for text classification and clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7673ebed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Imports the pandas library, which is fundamental for data manipulation and analysis in tabular format (DataFrames).\n",
    "# It is a powerful tool for working with structured data.\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "# Imports the 'fetch_20newsgroups' function from the 'datasets' module of the scikit-learn library.\n",
    "# This function is specifically used to download and load the famous '20 Newsgroups' dataset.\n",
    "\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "# Calls the 'fetch_20newsgroups' function to load the '20 Newsgroups' dataset.\n",
    "# 'subset='all'' specifies that we want to load all available data (both training and test sets).\n",
    "# 'remove=('headers', 'footers', 'quotes')' is an argument that instructs the function to remove common and less relevant parts of the newsgroup articles,\n",
    "# such as headers (sender info, date, etc.), footers (signatures), and quotes (parts of previous messages).\n",
    "# This is a common pre-processing step to focus on the main content of the text.\n",
    "# The result (the cleaned dataset) is stored in the 'newsgroups' variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4980fafc",
   "metadata": {},
   "source": [
    "After importing the data, the next step is to visualize it. To make it easier to understand during preprocessing, we will apply a style to the code, making the presentation of the data more pleasant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "267eb21c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_6c64b_row0_col0, #T_6c64b_row1_col0, #T_6c64b_row2_col0, #T_6c64b_row3_col0, #T_6c64b_row4_col0 {\n",
       "  background-color: lightgray;\n",
       "  text-align: left;\n",
       "  border-color: white;\n",
       "  font-size: 12pt;\n",
       "}\n",
       "#T_6c64b_row0_col1, #T_6c64b_row1_col1, #T_6c64b_row2_col1, #T_6c64b_row3_col1, #T_6c64b_row4_col1 {\n",
       "  background-color: lightblue;\n",
       "  text-align: left;\n",
       "  border-color: white;\n",
       "  font-size: 12pt;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_6c64b\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_6c64b_level0_col0\" class=\"col_heading level0 col0\" >Category</th>\n",
       "      <th id=\"T_6c64b_level0_col1\" class=\"col_heading level0 col1\" >News</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_6c64b_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_6c64b_row0_col0\" class=\"data row0 col0\" >rec.sport.hockey</td>\n",
       "      <td id=\"T_6c64b_row0_col1\" class=\"data row0 col1\" >\n",
       "\n",
       "I am sure some bashers of Pens fans are pretty confused about the lack\n",
       "of any kind of posts about the recent Pens massacre of the Devils. Actually,\n",
       "I am  bit puzzled too and a bit relieved. However, I am going to put an end\n",
       "to non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\n",
       "are killing those Devils worse than I thought. Jagr just showed you why\n",
       "he is much better than his regular season stats. He is also a lot\n",
       "fo fun to watch in the playoffs. Bowman should let JAgr have a lot of\n",
       "fun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\n",
       "regular season game.          PENS RULE!!!\n",
       "\n",
       "</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6c64b_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_6c64b_row1_col0\" class=\"data row1 col0\" >comp.sys.ibm.pc.hardware</td>\n",
       "      <td id=\"T_6c64b_row1_col1\" class=\"data row1 col1\" >My brother is in the market for a high-performance video card that supports\n",
       "VESA local bus with 1-2MB RAM.  Does anyone have suggestions/ideas on:\n",
       "\n",
       "  - Diamond Stealth Pro Local Bus\n",
       "\n",
       "  - Orchid Farenheit 1280\n",
       "\n",
       "  - ATI Graphics Ultra Pro\n",
       "\n",
       "  - Any other high-performance VLB card\n",
       "\n",
       "\n",
       "Please post or email.  Thank you!\n",
       "\n",
       "  - Matt\n",
       "</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6c64b_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_6c64b_row2_col0\" class=\"data row2 col0\" >talk.politics.mideast</td>\n",
       "      <td id=\"T_6c64b_row2_col1\" class=\"data row2 col1\" >\n",
       "\n",
       "\n",
       "\n",
       "\tFinally you said what you dream about. Mediterranean???? That was new....\n",
       "\tThe area will be \"greater\" after some years, like your \"holocaust\" numbers......\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\t\t*****\n",
       "\tIs't July in USA now????? Here in Sweden it's April and still cold.\n",
       "\tOr have you changed your calendar???\n",
       "\n",
       "\n",
       "\t\t\t\t\t\t    ****************\n",
       "\t\t\t\t\t\t    ******************\n",
       "\t\t\t    ***************\n",
       "\n",
       "\n",
       "\tNOTHING OF THE MENTIONED IS TRUE, BUT LET SAY IT's TRUE.\n",
       "\t\n",
       "\tSHALL THE AZERI WOMEN AND CHILDREN GOING TO PAY THE PRICE WITH\n",
       "\t\t\t\t\t\t    **************\n",
       "\tBEING RAPED, KILLED AND TORTURED BY THE ARMENIANS??????????\n",
       "\t\n",
       "\tHAVE YOU HEARDED SOMETHING CALLED: \"GENEVA CONVENTION\"???????\n",
       "\tYOU FACIST!!!!!\n",
       "\n",
       "\n",
       "\n",
       "\tOhhh i forgot, this is how Armenians fight, nobody has forgot\n",
       "\tyou killings, rapings and torture against the Kurds and Turks once\n",
       "\tupon a time!\n",
       "      \n",
       "       \n",
       "\n",
       "\n",
       "Ohhhh so swedish RedCross workers do lie they too? What ever you say\n",
       "\"regional killer\", if you don't like the person then shoot him that's your policy.....l\n",
       "\n",
       "\n",
       "\t\t\t\t\t\t\t\t\t\ti\n",
       "\t\t\t\t\t\t\t\t\t\ti\n",
       "\t\t\t\t\t\t\t\t\t\ti\n",
       "\tConfused?????\t\t\t\t\t\t\t\ti\n",
       "\t\t\t\t\t\t\t\t\t\ti\n",
       "        Search Turkish planes? You don't know what you are talking about.\ti\n",
       "        Turkey's government has announced that it's giving weapons  <-----------i\n",
       "        to Azerbadjan since Armenia started to attack Azerbadjan\t\t\n",
       "        it self, not the Karabag province. So why search a plane for weapons\t\n",
       "        since it's content is announced to be weapons?   \n",
       "\n",
       "\tIf there is one that's confused then that's you! We have the right (and we do)\n",
       "\tto give weapons to the Azeris, since Armenians started the fight in Azerbadjan!\n",
       " \n",
       "\n",
       "\n",
       "\tShoot down with what? Armenian bread and butter? Or the arms and personel \n",
       "\tof the Russian army?\n",
       "\n",
       "\n",
       "</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6c64b_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_6c64b_row3_col0\" class=\"data row3 col0\" >comp.sys.ibm.pc.hardware</td>\n",
       "      <td id=\"T_6c64b_row3_col1\" class=\"data row3 col1\" >\n",
       "Think!\n",
       "\n",
       "It's the SCSI card doing the DMA transfers NOT the disks...\n",
       "\n",
       "The SCSI card can do DMA transfers containing data from any of the SCSI devices\n",
       "it is attached when it wants to.\n",
       "\n",
       "An important feature of SCSI is the ability to detach a device. This frees the\n",
       "SCSI bus for other devices. This is typically used in a multi-tasking OS to\n",
       "start transfers on several devices. While each device is seeking the data the\n",
       "bus is free for other commands and data transfers. When the devices are\n",
       "ready to transfer the data they can aquire the bus and send the data.\n",
       "\n",
       "On an IDE bus when you start a transfer the bus is busy until the disk has seeked\n",
       "the data and transfered it. This is typically a 10-20ms second lock out for other\n",
       "processes wanting the bus irrespective of transfer time.\n",
       "</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6c64b_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_6c64b_row4_col0\" class=\"data row4 col0\" >comp.sys.mac.hardware</td>\n",
       "      <td id=\"T_6c64b_row4_col1\" class=\"data row4 col1\" >1)    I have an old Jasmine drive which I cannot use with my new system.\n",
       " My understanding is that I have to upsate the driver with a more modern\n",
       "one in order to gain compatability with system 7.0.1.  does anyone know\n",
       "of an inexpensive program to do this?  ( I have seen formatters for <$20\n",
       "buit have no idea if they will work)\n",
       " \n",
       "2)     I have another ancient device, this one a tape drive for which\n",
       "the back utility freezes the system if I try to use it.  THe drive is a\n",
       "jasmine direct tape (bought used for $150 w/ 6 tapes, techmar\n",
       "mechanism).  Essentially I have the same question as above, anyone know\n",
       "of an inexpensive beckup utility I can use with system 7.0.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1c37a1a5810>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Imports the pandas library for data manipulation and analysis (DataFrames).\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "# Imports 'fetch_20newsgroups' to load the 20 Newsgroups dataset from scikit-learn.\n",
    "\n",
    "# Load the 20 Newsgroups dataset\n",
    "# Loads all subsets of the dataset and removes headers, footers, and quotes\n",
    "# to focus on the main content for NLP tasks.\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Creating a pandas DataFrame from the newsgroups data\n",
    "# Creates a DataFrame with two columns: 'Category' (human-readable names)\n",
    "# and 'News' (the cleaned text content).\n",
    "data_frame = pd.DataFrame({\n",
    "    'Category': [newsgroups.target_names[i] for i in newsgroups.target],\n",
    "    'News': newsgroups.data\n",
    "})\n",
    "\n",
    "# Function to style the DataFrame rows\n",
    "# Defines a function to apply alternating background colors (lightblue/lightgray) to rows\n",
    "# for improved visual readability.\n",
    "def highlight_alternating_rows(row):\n",
    "    return ['background-color: lightblue' if i % 2 else 'background-color: lightgray' for i in range(len(row))]\n",
    "\n",
    "# Apply styling to the first few rows of the DataFrame\n",
    "# Selects the first 5 rows (.head()), applies the alternating row highlight,\n",
    "# and sets common CSS properties for text alignment, border color, and font size.\n",
    "# The .hide_index() method (commented out) was removed due to compatibility with older pandas versions.\n",
    "styled_data_frame = data_frame.head().style.apply(highlight_alternating_rows, axis=1).set_properties(**{\n",
    "    'text-align': 'left',\n",
    "    'border-color': 'white',\n",
    "    'font-size': '12pt'\n",
    "})\n",
    "\n",
    "# Display the styled DataFrame\n",
    "# Renders the visually enhanced DataFrame in an interactive environment (e.g., Jupyter Notebook).\n",
    "styled_data_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117670aa",
   "metadata": {},
   "source": [
    "After loading the news dataset, the next step is to load and store the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f11940b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'being', 'can', 'll', 'more', \"we've\", 'is', \"you'd\", \"mightn't\", 'why', 'all', 'its', \"hasn't\", 'which', \"won't\", \"you'll\", 'mightn', 'yourself', \"you've\", 'after', 'ain', 'own', 'we', \"aren't\", 'about', 'are', \"you're\", 'other', 'again', 'both', 'him', 'when', 'your', 'those', \"hadn't\", 'didn', 'there', 'any', \"she'll\", 'needn', 'his', 'do', 'does', \"he's\", \"don't\", 'here', 'this', 'off', 'yourselves', 'them', 'nor', 'by', \"she'd\", 'some', 'have', 'with', 'into', 'o', 'did', \"he'll\", \"haven't\", 'herself', 'has', 've', 're', 'wouldn', \"needn't\", 'hadn', 'few', 'if', 'm', \"they'd\", 'her', 'doing', 'himself', 'they', \"shouldn't\", 'y', 'while', 'hers', \"doesn't\", 'too', 'how', 'but', 'a', \"didn't\", 'were', \"we're\", 'ours', \"we'll\", 'i', 'no', 'and', \"i'll\", \"shan't\", 'of', 'most', 'he', 'because', 'having', 'out', \"they'll\", 't', 'so', 'over', 'theirs', 'don', \"should've\", \"weren't\", 'an', \"they've\", 'very', 's', 'where', \"it'll\", 'itself', 'weren', \"couldn't\", 'only', 'or', 'at', 'was', 'until', 'she', 'who', 'been', 'above', 'what', 'in', \"mustn't\", 'through', 'my', \"wouldn't\", 'each', 'whom', \"that'll\", 'wasn', 'myself', 'shan', \"it'd\", \"i've\", 'just', 'themselves', \"wasn't\", 'doesn', 'during', 'couldn', 'am', 'should', 'the', 'to', 'from', 'up', 'ma', 'ourselves', 'once', 'now', 'shouldn', 'then', 'me', 'as', 'isn', 'than', 'our', 'won', 'it', 'for', 'below', 'down', 'yours', 'that', 'before', 'on', 'd', \"she's\", \"we'd\", 'haven', 'further', 'will', \"he'd\", 'aren', 'you', 'be', 'against', 'between', 'mustn', 'under', \"i'd\", \"it's\", \"isn't\", 'not', 'same', \"they're\", 'their', 'these', 'hasn', \"i'm\", 'had', 'such'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Felipe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download the stopwords corpus (if not already downloaded).\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the English stopwords into a set for efficient lookup.\n",
    "# Change 'english' to 'portuguese' for Portuguese stopwords.\n",
    "english_stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Print the loaded set of stopwords.\n",
    "print(english_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a64c307",
   "metadata": {},
   "source": [
    "We have the `20newsgroups` (full texts) and the `stopwords` set (words to remove). To apply the removal, we first need to \"break\" the `20newsgroups` texts into individual words (tokenize). Then, we will create a function that tokenizes any text and removes the stopwords found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5c332f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# Ensure you have 'punkt' tokenizer downloaded: nltk.download('punkt')\n",
    "# Ensure you have 'stopwords' downloaded: nltk.download('stopwords')\n",
    "\n",
    "# Assuming 'stop_words' (e.g., english_stop_words or portuguese_stop_words)\n",
    "# has been previously loaded as a set of stopwords.\n",
    "# Example: from nltk.corpus import stopwords\n",
    "#          stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(input_text):\n",
    "    \"\"\"\n",
    "    Removes stopwords from a given text.\n",
    "\n",
    "    Args:\n",
    "        input_text (str): The input text string from which to remove stopwords.\n",
    "\n",
    "    Returns:\n",
    "        str: The text string with stopwords removed, where words are joined by spaces.\n",
    "    \"\"\"\n",
    "    # Tokenizes the input text into a list of individual words (tokens).\n",
    "    # nltk.word_tokenize handles punctuation correctly by separating it from words.\n",
    "    words = nltk.word_tokenize(input_text)\n",
    "\n",
    "    # Filters the tokenized words, keeping only those that are NOT in the 'stop_words' set.\n",
    "    # word.lower() is used to ensure case-insensitive matching against the stop_words set.\n",
    "    filtered_words = [word for word in words if word.lower() not in english_stop_words]\n",
    "\n",
    "    # Joins the filtered words back into a single string, separated by spaces.\n",
    "    # This reconstructs the text without the stopwords.\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Example Usage (assuming stop_words is defined, e.g., english_stop_words from previous steps)\n",
    "# sample_text = \"This is a sample sentence showing the removal of common words.\"\n",
    "# cleaned_text = remove_stopwords(sample_text)\n",
    "# print(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a5d95e",
   "metadata": {},
   "source": [
    "To demonstrate the use of the function, we will apply it to the first text in the 20newsgroups dataset, displaying the original text and the text after removing the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db4c29f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text\n",
      "----------------\n",
      "\n",
      "\n",
      "I am sure some bashers of Pens fans are pretty confused about the lack\n",
      "of any kind of posts about the recent Pens massacre of the Devils. Actually,\n",
      "I am  bit puzzled too and a bit relieved. However, I am going to put an end\n",
      "to non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\n",
      "are killing those Devils worse than I thought. Jagr just showed you why\n",
      "he is much better than his regular season stats. He is also a lot\n",
      "fo fun to watch in the playoffs. Bowman should let JAgr have a lot of\n",
      "fun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\n",
      "regular season game.          PENS RULE!!!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Text After Stopwords Removal:\n",
      "---------------------------------\n",
      "sure bashers Pens fans pretty confused lack kind posts recent Pens massacre Devils . Actually , bit puzzled bit relieved . However , going put end non-PIttsburghers ' relief bit praise Pens . Man , killing Devils worse thought . Jagr showed much better regular season stats . also lot fo fun watch playoffs . Bowman let JAgr lot fun next couple games since Pens going beat pulp Jersey anyway . disappointed see Islanders lose final regular season game . PENS RULE ! ! !\n"
     ]
    }
   ],
   "source": [
    "# Get the first document text from the loaded newsgroups dataset.\n",
    "original_text = newsgroups.data[0]\n",
    "\n",
    "# Print a header indicating the original text.\n",
    "print(\"Original Text\")\n",
    "print(\"----------------\")\n",
    "# Display the original text content.\n",
    "print(original_text)\n",
    "\n",
    "# Print a header for the text after stopword removal.\n",
    "print(\"\\n\\nText After Stopwords Removal:\")\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "# Call the 'remove_stopwords' function to process the original text\n",
    "# and print the cleaned result.\n",
    "print(remove_stopwords(original_text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
