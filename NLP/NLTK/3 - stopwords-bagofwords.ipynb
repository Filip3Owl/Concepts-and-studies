{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45544a79",
   "metadata": {},
   "source": [
    "# STOPWORDS\n",
    "\n",
    "Stopwords are common words (e.g. \"is\", \"the\") that are removed in NLP because they do not contribute significantly to the meaning of the text. This preprocessing aims to eliminate irrelevant and auxiliary terms, focusing the analysis on the essential content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20155a8d",
   "metadata": {},
   "source": [
    "Removing stopwords simplifies sentences by eliminating common words like **\"I\", \"have\", \"been\", \"the\", \"and\", \"that\", \"are\", \"about\", \"of\", focusing on key terms such as \"studied\", \"effects\", \"global warming\", \"noticed\", \"studies\", \"inconclusive\", \"results\"**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d130bcaa",
   "metadata": {},
   "source": [
    "Stopword extraction (or removal) offers several key benefits in Natural Language Processing (NLP):\n",
    "\n",
    "1. **Dimensionality Reduction:** Reduces the total number of unique words (vocabulary) in a corpus, which optimizes memory usage and speeds up processing in ML models.\n",
    "2. **Focus on Relevant Words:** Allows NLP algorithms to focus on terms that actually carry meaning and contextual information, improving the relevance of the analysis.\n",
    "3. **Improved Efficiency and Performance:** By dealing with less data, algorithms run faster and more efficiently, which is crucial for large volumes of text.\n",
    "4. **Increased Accuracy in Specific Tasks:** In tasks such as text classification, sentiment analysis, and information retrieval, stopword removal can improve the accuracy of results, as irrelevant words do not \"pollute\" the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdd0cd1",
   "metadata": {},
   "source": [
    "For the practice of stopword removal, we will use NLTK's 20Newsgroups corpus. This dataset, consisting of about 20,000 articles from old online forums (newsgroups) divided into 20 categories, is widely used in machine learning for text classification and clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7673ebed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Imports the pandas library, which is fundamental for data manipulation and analysis in tabular format (DataFrames).\n",
    "# It is a powerful tool for working with structured data.\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "# Imports the 'fetch_20newsgroups' function from the 'datasets' module of the scikit-learn library.\n",
    "# This function is specifically used to download and load the famous '20 Newsgroups' dataset.\n",
    "\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "# Calls the 'fetch_20newsgroups' function to load the '20 Newsgroups' dataset.\n",
    "# 'subset='all'' specifies that we want to load all available data (both training and test sets).\n",
    "# 'remove=('headers', 'footers', 'quotes')' is an argument that instructs the function to remove common and less relevant parts of the newsgroup articles,\n",
    "# such as headers (sender info, date, etc.), footers (signatures), and quotes (parts of previous messages).\n",
    "# This is a common pre-processing step to focus on the main content of the text.\n",
    "# The result (the cleaned dataset) is stored in the 'newsgroups' variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4980fafc",
   "metadata": {},
   "source": [
    "After importing the data, the next step is to visualize it. To make it easier to understand during preprocessing, we will apply a style to the code, making the presentation of the data more pleasant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "267eb21c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_6c64b_row0_col0, #T_6c64b_row1_col0, #T_6c64b_row2_col0, #T_6c64b_row3_col0, #T_6c64b_row4_col0 {\n",
       "  background-color: lightgray;\n",
       "  text-align: left;\n",
       "  border-color: white;\n",
       "  font-size: 12pt;\n",
       "}\n",
       "#T_6c64b_row0_col1, #T_6c64b_row1_col1, #T_6c64b_row2_col1, #T_6c64b_row3_col1, #T_6c64b_row4_col1 {\n",
       "  background-color: lightblue;\n",
       "  text-align: left;\n",
       "  border-color: white;\n",
       "  font-size: 12pt;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_6c64b\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_6c64b_level0_col0\" class=\"col_heading level0 col0\" >Category</th>\n",
       "      <th id=\"T_6c64b_level0_col1\" class=\"col_heading level0 col1\" >News</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_6c64b_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_6c64b_row0_col0\" class=\"data row0 col0\" >rec.sport.hockey</td>\n",
       "      <td id=\"T_6c64b_row0_col1\" class=\"data row0 col1\" >\n",
       "\n",
       "I am sure some bashers of Pens fans are pretty confused about the lack\n",
       "of any kind of posts about the recent Pens massacre of the Devils. Actually,\n",
       "I am  bit puzzled too and a bit relieved. However, I am going to put an end\n",
       "to non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\n",
       "are killing those Devils worse than I thought. Jagr just showed you why\n",
       "he is much better than his regular season stats. He is also a lot\n",
       "fo fun to watch in the playoffs. Bowman should let JAgr have a lot of\n",
       "fun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\n",
       "regular season game.          PENS RULE!!!\n",
       "\n",
       "</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6c64b_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_6c64b_row1_col0\" class=\"data row1 col0\" >comp.sys.ibm.pc.hardware</td>\n",
       "      <td id=\"T_6c64b_row1_col1\" class=\"data row1 col1\" >My brother is in the market for a high-performance video card that supports\n",
       "VESA local bus with 1-2MB RAM.  Does anyone have suggestions/ideas on:\n",
       "\n",
       "  - Diamond Stealth Pro Local Bus\n",
       "\n",
       "  - Orchid Farenheit 1280\n",
       "\n",
       "  - ATI Graphics Ultra Pro\n",
       "\n",
       "  - Any other high-performance VLB card\n",
       "\n",
       "\n",
       "Please post or email.  Thank you!\n",
       "\n",
       "  - Matt\n",
       "</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6c64b_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_6c64b_row2_col0\" class=\"data row2 col0\" >talk.politics.mideast</td>\n",
       "      <td id=\"T_6c64b_row2_col1\" class=\"data row2 col1\" >\n",
       "\n",
       "\n",
       "\n",
       "\tFinally you said what you dream about. Mediterranean???? That was new....\n",
       "\tThe area will be \"greater\" after some years, like your \"holocaust\" numbers......\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\t\t*****\n",
       "\tIs't July in USA now????? Here in Sweden it's April and still cold.\n",
       "\tOr have you changed your calendar???\n",
       "\n",
       "\n",
       "\t\t\t\t\t\t    ****************\n",
       "\t\t\t\t\t\t    ******************\n",
       "\t\t\t    ***************\n",
       "\n",
       "\n",
       "\tNOTHING OF THE MENTIONED IS TRUE, BUT LET SAY IT's TRUE.\n",
       "\t\n",
       "\tSHALL THE AZERI WOMEN AND CHILDREN GOING TO PAY THE PRICE WITH\n",
       "\t\t\t\t\t\t    **************\n",
       "\tBEING RAPED, KILLED AND TORTURED BY THE ARMENIANS??????????\n",
       "\t\n",
       "\tHAVE YOU HEARDED SOMETHING CALLED: \"GENEVA CONVENTION\"???????\n",
       "\tYOU FACIST!!!!!\n",
       "\n",
       "\n",
       "\n",
       "\tOhhh i forgot, this is how Armenians fight, nobody has forgot\n",
       "\tyou killings, rapings and torture against the Kurds and Turks once\n",
       "\tupon a time!\n",
       "      \n",
       "       \n",
       "\n",
       "\n",
       "Ohhhh so swedish RedCross workers do lie they too? What ever you say\n",
       "\"regional killer\", if you don't like the person then shoot him that's your policy.....l\n",
       "\n",
       "\n",
       "\t\t\t\t\t\t\t\t\t\ti\n",
       "\t\t\t\t\t\t\t\t\t\ti\n",
       "\t\t\t\t\t\t\t\t\t\ti\n",
       "\tConfused?????\t\t\t\t\t\t\t\ti\n",
       "\t\t\t\t\t\t\t\t\t\ti\n",
       "        Search Turkish planes? You don't know what you are talking about.\ti\n",
       "        Turkey's government has announced that it's giving weapons  <-----------i\n",
       "        to Azerbadjan since Armenia started to attack Azerbadjan\t\t\n",
       "        it self, not the Karabag province. So why search a plane for weapons\t\n",
       "        since it's content is announced to be weapons?   \n",
       "\n",
       "\tIf there is one that's confused then that's you! We have the right (and we do)\n",
       "\tto give weapons to the Azeris, since Armenians started the fight in Azerbadjan!\n",
       " \n",
       "\n",
       "\n",
       "\tShoot down with what? Armenian bread and butter? Or the arms and personel \n",
       "\tof the Russian army?\n",
       "\n",
       "\n",
       "</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6c64b_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_6c64b_row3_col0\" class=\"data row3 col0\" >comp.sys.ibm.pc.hardware</td>\n",
       "      <td id=\"T_6c64b_row3_col1\" class=\"data row3 col1\" >\n",
       "Think!\n",
       "\n",
       "It's the SCSI card doing the DMA transfers NOT the disks...\n",
       "\n",
       "The SCSI card can do DMA transfers containing data from any of the SCSI devices\n",
       "it is attached when it wants to.\n",
       "\n",
       "An important feature of SCSI is the ability to detach a device. This frees the\n",
       "SCSI bus for other devices. This is typically used in a multi-tasking OS to\n",
       "start transfers on several devices. While each device is seeking the data the\n",
       "bus is free for other commands and data transfers. When the devices are\n",
       "ready to transfer the data they can aquire the bus and send the data.\n",
       "\n",
       "On an IDE bus when you start a transfer the bus is busy until the disk has seeked\n",
       "the data and transfered it. This is typically a 10-20ms second lock out for other\n",
       "processes wanting the bus irrespective of transfer time.\n",
       "</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_6c64b_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_6c64b_row4_col0\" class=\"data row4 col0\" >comp.sys.mac.hardware</td>\n",
       "      <td id=\"T_6c64b_row4_col1\" class=\"data row4 col1\" >1)    I have an old Jasmine drive which I cannot use with my new system.\n",
       " My understanding is that I have to upsate the driver with a more modern\n",
       "one in order to gain compatability with system 7.0.1.  does anyone know\n",
       "of an inexpensive program to do this?  ( I have seen formatters for <$20\n",
       "buit have no idea if they will work)\n",
       " \n",
       "2)     I have another ancient device, this one a tape drive for which\n",
       "the back utility freezes the system if I try to use it.  THe drive is a\n",
       "jasmine direct tape (bought used for $150 w/ 6 tapes, techmar\n",
       "mechanism).  Essentially I have the same question as above, anyone know\n",
       "of an inexpensive beckup utility I can use with system 7.0.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1c37a1a5810>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Imports the pandas library for data manipulation and analysis (DataFrames).\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "# Imports 'fetch_20newsgroups' to load the 20 Newsgroups dataset from scikit-learn.\n",
    "\n",
    "# Load the 20 Newsgroups dataset\n",
    "# Loads all subsets of the dataset and removes headers, footers, and quotes\n",
    "# to focus on the main content for NLP tasks.\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Creating a pandas DataFrame from the newsgroups data\n",
    "# Creates a DataFrame with two columns: 'Category' (human-readable names)\n",
    "# and 'News' (the cleaned text content).\n",
    "data_frame = pd.DataFrame({\n",
    "    'Category': [newsgroups.target_names[i] for i in newsgroups.target],\n",
    "    'News': newsgroups.data\n",
    "})\n",
    "\n",
    "# Function to style the DataFrame rows\n",
    "# Defines a function to apply alternating background colors (lightblue/lightgray) to rows\n",
    "# for improved visual readability.\n",
    "def highlight_alternating_rows(row):\n",
    "    return ['background-color: lightblue' if i % 2 else 'background-color: lightgray' for i in range(len(row))]\n",
    "\n",
    "# Apply styling to the first few rows of the DataFrame\n",
    "# Selects the first 5 rows (.head()), applies the alternating row highlight,\n",
    "# and sets common CSS properties for text alignment, border color, and font size.\n",
    "# The .hide_index() method (commented out) was removed due to compatibility with older pandas versions.\n",
    "styled_data_frame = data_frame.head().style.apply(highlight_alternating_rows, axis=1).set_properties(**{\n",
    "    'text-align': 'left',\n",
    "    'border-color': 'white',\n",
    "    'font-size': '12pt'\n",
    "})\n",
    "\n",
    "# Display the styled DataFrame\n",
    "# Renders the visually enhanced DataFrame in an interactive environment (e.g., Jupyter Notebook).\n",
    "styled_data_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117670aa",
   "metadata": {},
   "source": [
    "After loading the news dataset, the next step is to load and store the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f11940b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'being', 'can', 'll', 'more', \"we've\", 'is', \"you'd\", \"mightn't\", 'why', 'all', 'its', \"hasn't\", 'which', \"won't\", \"you'll\", 'mightn', 'yourself', \"you've\", 'after', 'ain', 'own', 'we', \"aren't\", 'about', 'are', \"you're\", 'other', 'again', 'both', 'him', 'when', 'your', 'those', \"hadn't\", 'didn', 'there', 'any', \"she'll\", 'needn', 'his', 'do', 'does', \"he's\", \"don't\", 'here', 'this', 'off', 'yourselves', 'them', 'nor', 'by', \"she'd\", 'some', 'have', 'with', 'into', 'o', 'did', \"he'll\", \"haven't\", 'herself', 'has', 've', 're', 'wouldn', \"needn't\", 'hadn', 'few', 'if', 'm', \"they'd\", 'her', 'doing', 'himself', 'they', \"shouldn't\", 'y', 'while', 'hers', \"doesn't\", 'too', 'how', 'but', 'a', \"didn't\", 'were', \"we're\", 'ours', \"we'll\", 'i', 'no', 'and', \"i'll\", \"shan't\", 'of', 'most', 'he', 'because', 'having', 'out', \"they'll\", 't', 'so', 'over', 'theirs', 'don', \"should've\", \"weren't\", 'an', \"they've\", 'very', 's', 'where', \"it'll\", 'itself', 'weren', \"couldn't\", 'only', 'or', 'at', 'was', 'until', 'she', 'who', 'been', 'above', 'what', 'in', \"mustn't\", 'through', 'my', \"wouldn't\", 'each', 'whom', \"that'll\", 'wasn', 'myself', 'shan', \"it'd\", \"i've\", 'just', 'themselves', \"wasn't\", 'doesn', 'during', 'couldn', 'am', 'should', 'the', 'to', 'from', 'up', 'ma', 'ourselves', 'once', 'now', 'shouldn', 'then', 'me', 'as', 'isn', 'than', 'our', 'won', 'it', 'for', 'below', 'down', 'yours', 'that', 'before', 'on', 'd', \"she's\", \"we'd\", 'haven', 'further', 'will', \"he'd\", 'aren', 'you', 'be', 'against', 'between', 'mustn', 'under', \"i'd\", \"it's\", \"isn't\", 'not', 'same', \"they're\", 'their', 'these', 'hasn', \"i'm\", 'had', 'such'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Felipe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download the stopwords corpus (if not already downloaded).\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the English stopwords into a set for efficient lookup.\n",
    "# Change 'english' to 'portuguese' for Portuguese stopwords.\n",
    "english_stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Print the loaded set of stopwords.\n",
    "print(english_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a64c307",
   "metadata": {},
   "source": [
    "We have the `20newsgroups` (full texts) and the `stopwords` set (words to remove). To apply the removal, we first need to \"break\" the `20newsgroups` texts into individual words (tokenize). Then, we will create a function that tokenizes any text and removes the stopwords found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5c332f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# Ensure you have 'punkt' tokenizer downloaded: nltk.download('punkt')\n",
    "# Ensure you have 'stopwords' downloaded: nltk.download('stopwords')\n",
    "\n",
    "# Assuming 'stop_words' (e.g., english_stop_words or portuguese_stop_words)\n",
    "# has been previously loaded as a set of stopwords.\n",
    "# Example: from nltk.corpus import stopwords\n",
    "#          stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(input_text):\n",
    "    \"\"\"\n",
    "    Removes stopwords from a given text.\n",
    "\n",
    "    Args:\n",
    "        input_text (str): The input text string from which to remove stopwords.\n",
    "\n",
    "    Returns:\n",
    "        str: The text string with stopwords removed, where words are joined by spaces.\n",
    "    \"\"\"\n",
    "    # Tokenizes the input text into a list of individual words (tokens).\n",
    "    # nltk.word_tokenize handles punctuation correctly by separating it from words.\n",
    "    words = nltk.word_tokenize(input_text)\n",
    "\n",
    "    # Filters the tokenized words, keeping only those that are NOT in the 'stop_words' set.\n",
    "    # word.lower() is used to ensure case-insensitive matching against the stop_words set.\n",
    "    filtered_words = [word for word in words if word.lower() not in english_stop_words]\n",
    "\n",
    "    # Joins the filtered words back into a single string, separated by spaces.\n",
    "    # This reconstructs the text without the stopwords.\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Example Usage (assuming stop_words is defined, e.g., english_stop_words from previous steps)\n",
    "# sample_text = \"This is a sample sentence showing the removal of common words.\"\n",
    "# cleaned_text = remove_stopwords(sample_text)\n",
    "# print(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a5d95e",
   "metadata": {},
   "source": [
    "To demonstrate the use of the function, we will apply it to the first text in the 20newsgroups dataset, displaying the original text and the text after removing the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db4c29f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text\n",
      "----------------\n",
      "\n",
      "\n",
      "I am sure some bashers of Pens fans are pretty confused about the lack\n",
      "of any kind of posts about the recent Pens massacre of the Devils. Actually,\n",
      "I am  bit puzzled too and a bit relieved. However, I am going to put an end\n",
      "to non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\n",
      "are killing those Devils worse than I thought. Jagr just showed you why\n",
      "he is much better than his regular season stats. He is also a lot\n",
      "fo fun to watch in the playoffs. Bowman should let JAgr have a lot of\n",
      "fun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\n",
      "regular season game.          PENS RULE!!!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Text After Stopwords Removal:\n",
      "---------------------------------\n",
      "sure bashers Pens fans pretty confused lack kind posts recent Pens massacre Devils . Actually , bit puzzled bit relieved . However , going put end non-PIttsburghers ' relief bit praise Pens . Man , killing Devils worse thought . Jagr showed much better regular season stats . also lot fo fun watch playoffs . Bowman let JAgr lot fun next couple games since Pens going beat pulp Jersey anyway . disappointed see Islanders lose final regular season game . PENS RULE ! ! !\n"
     ]
    }
   ],
   "source": [
    "# Get the first document text from the loaded newsgroups dataset.\n",
    "original_text = newsgroups.data[0]\n",
    "\n",
    "# Print a header indicating the original text.\n",
    "print(\"Original Text\")\n",
    "print(\"----------------\")\n",
    "# Display the original text content.\n",
    "print(original_text)\n",
    "\n",
    "# Print a header for the text after stopword removal.\n",
    "print(\"\\n\\nText After Stopwords Removal:\")\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "# Call the 'remove_stopwords' function to process the original text\n",
    "# and print the cleaned result.\n",
    "print(remove_stopwords(original_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44578816",
   "metadata": {},
   "source": [
    "Removing stopwords in the example demonstrated that the text is reduced without losing meaning. We will now apply this batch preprocessing to the entire 20Newsgroups dataset to achieve the expected benefits for large text sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc8ba85",
   "metadata": {},
   "source": [
    "The primary initial benefit of stopword removal is data size reduction on disk. By eliminating frequent words with little intrinsic meaning, we decrease the dataset's volume. We will verify this in practice by comparing the size before and after removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08be092b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Felipe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Total Words: 3423145\n",
      "Total Words After Stopword Removal: 3281948\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download NLTK stopwords (if not already present).\n",
    "nltk.download('stopwords')\n",
    "# Load Portuguese stopwords into a set for efficient lookup.\n",
    "# Using 'portuguese' as specified in the original code.\n",
    "stop_words = set(stopwords.words('portuguese'))\n",
    "\n",
    "# Load the 20 Newsgroups dataset.\n",
    "# 'subset='all'' gets both training and test data.\n",
    "# 'remove' strips headers, footers, and quotes for cleaner text.\n",
    "news_groups_data = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Calculate the total number of words in the original dataset.\n",
    "# It splits each document into words and sums their counts across all documents.\n",
    "original_word_count = sum(len(text.split()) for text in news_groups_data.data)\n",
    "\n",
    "# Process each document: split into words, filter out stopwords, then join back into a string.\n",
    "# This creates a new list of documents with stopwords removed.\n",
    "cleaned_data = [\" \".join(word for word in text.split() if word.lower() not in stop_words) for text in news_groups_data.data]\n",
    "\n",
    "# Calculate the total number of words in the cleaned dataset.\n",
    "# This reflects the word count after stopword removal.\n",
    "cleaned_word_count = sum(len(text.split()) for text in cleaned_data)\n",
    "\n",
    "# Print the comparison of word counts before and after stopword removal.\n",
    "print(f\"Original Total Words: {original_word_count}\")\n",
    "print(f\"Total Words After Stopword Removal: {cleaned_word_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f34fc3",
   "metadata": {},
   "source": [
    "\n",
    "Analyzing the results, the **original dataset contained 3,423,145 words**. After **removing stopwords, this number dropped to 3,281,948**, a **reduction of 141,197 words**.\n",
    "\n",
    "This is from a dataset of 20,000 texts, but in real-world NLP applications, datasets are often much larger. This demonstrates that **stopword removal can have a substantial impact on overall dataset size**, leading to more efficient processing and storage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750091c9",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "Stopword removal is **crucial for eliminating noise in text analysis**. Consider an application tracking trending terms over time. Without filtering stopwords, **irrelevant words could obscure valuable insights**, such as identifying prominent politicians or the most discussed electronics.\n",
    "\n",
    "We'll now demonstrate this by showing the most frequent words in the 20Newsgroups dataset, first from the original text, and then after stopword removal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "062ae644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Most Common Words (Original Text):\n",
      "[('the', 153574), ('to', 83965), ('of', 75111), ('a', 65521), ('and', 64420), ('in', 45448), ('is', 45404), ('I', 44740), ('that', 41057), ('for', 29222)]\n",
      "\n",
      "Top 10 Most Common Words (After Cleaning):\n",
      "[('AX', 62500), ('X', 12208), ('would', 9902), ('Q', 9313), ('one', 9197), ('W', 8546), ('F', 7940), ('G', 7598), ('B', 7576), ('R', 7530)]\n",
      "\n",
      "--- Explicit Document Test ---\n",
      "\n",
      "Original Text Sample:\n",
      "\n",
      "\n",
      "I am sur\n",
      "\n",
      "Cleaned Text Sample:\n",
      "sure bashe\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Load the ENGLISH stopwords into a set.\n",
    "# This is the crucial correction: using 'english' for English text.\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# --- 2. Carregar o Dataset 20 Newsgroups ---\n",
    "# Load the 20 Newsgroups dataset, removing headers, footers, and quotes.\n",
    "newsgroups_data = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# --- 3. Definir a Função de Limpeza (com comentários simples em inglês) ---\n",
    "def clean_text(text):\n",
    "    # Remove non-alphabetic characters (numbers, punctuation) and replaces them with a space.\n",
    "    # This also helps separate words that might be joined by punctuation.\n",
    "    cleaned_alpha_text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "    # Split the text into words, convert to lowercase, and filter out stopwords.\n",
    "    # The split() method without arguments handles multiple spaces correctly.\n",
    "    filtered_words = [word for word in cleaned_alpha_text.split() if word.lower() not in stop_words]\n",
    "    # Join the cleaned words back into a single string.\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# --- 4. Aplicar a Limpeza e Calcular Frequências ---\n",
    "\n",
    "# Clean all documents in the dataset.\n",
    "cleaned_documents = [clean_text(text) for text in newsgroups_data.data]\n",
    "\n",
    "# Calculate word frequencies for the original dataset.\n",
    "# Join all original texts into one string, then split by space to get words, then count.\n",
    "word_counts_original = Counter(\" \".join(news_groups_data.data).split())\n",
    "\n",
    "# Calculate word frequencies for the cleaned dataset.\n",
    "# Join all cleaned documents into one string, then split by space, then count.\n",
    "word_counts_cleaned = Counter(\" \".join(cleaned_documents).split())\n",
    "\n",
    "# --- 5. Imprimir os Resultados ---\n",
    "\n",
    "# Print the 10 most common words in the original text.\n",
    "print(\"Top 10 Most Common Words (Original Text):\")\n",
    "print(word_counts_original.most_common(10))\n",
    "\n",
    "# Print the 10 most common words after cleaning (stopword removal and non-alphabetic characters).\n",
    "print(\"\\nTop 10 Most Common Words (After Cleaning):\")\n",
    "print(word_counts_cleaned.most_common(10))\n",
    "\n",
    "# --- Teste explícito com um documento ---\n",
    "print(\"\\n--- Explicit Document Test ---\")\n",
    "sample_original_text = newsgroups_data.data[0]\n",
    "print(\"\\nOriginal Text Sample:\")\n",
    "print(sample_original_text[:10]) # Print first 500 chars for brevity\n",
    "\n",
    "sample_cleaned_text = clean_text(sample_original_text)\n",
    "print(\"\\nCleaned Text Sample:\")\n",
    "print(sample_cleaned_text[:10]) # Print first 500 chars of cleaned text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3886da77",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Summary: Model Evaluation Metrics from Confusion Matrix\n",
    "\n",
    "When evaluating classification models, understanding their **accuracy and reliability** is crucial. The **confusion matrix** is a vital tool for deriving key performance metrics, including **precision, recall, F-measure, and accuracy**.\n",
    "\n",
    "**Accuracy** is one of the most intuitive and widely used metrics. It represents the **proportion of correct predictions** out of the total. For instance, if a model correctly classifies 90 out of 100 samples, its accuracy is 90%. It offers a quick overview of the model's overall effectiveness.\n",
    "\n",
    "Beyond accuracy, the **F-measure (or F-score)** is another fundamental metric that **combines precision and recall** into a single score. It's particularly useful when dealing with **imbalanced classes**. A high F-score indicates a good balance between precision and recall. While both accuracy and F-measure provide valuable insights, it's essential to consider other metrics and the specific context when assessing model performance.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6136c9",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Model Evaluation Steps\n",
    "\n",
    "| Step | Description | Purpose |\n",
    "| :--- | :---------- | :------ |\n",
    "| **1. Load Dataset** | Load the `20 Newsgroups` base dataset. | To obtain the raw text data for analysis. |\n",
    "| **2. Clean Special Characters** | Remove special characters (e.g., `!@#$%^&*`). | To reduce noise and prepare text for tokenization and further processing. |\n",
    "| **3. Create Cleaned Dataset** | Create a dataset containing **only the cleaned data** (without stopwords removed yet). | To serve as a baseline for comparison, reflecting text after basic normalization. |\n",
    "| **4. Create Stopword-Removed Dataset** | Create a second dataset where **stopwords are also removed** from the cleaned data. | To isolate the impact of stopword removal on model performance and data size. |\n",
    "| **5. Execute ML Algorithm** | Run the machine learning algorithm on **both** the cleaned dataset and the stopword-removed dataset. | To train and test models under different pre-processing conditions. |\n",
    "| **6. Collect Metrics** | Collect **accuracy and F-measure** metrics for models trained on both datasets. | To compare model performance and determine the effectiveness of stopword removal on classification quality. |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b70bc8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Felipe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups # Imports the function to load the 20 Newsgroups dataset.\n",
    "from sklearn.feature_extraction.text import CountVectorizer # Imports CountVectorizer, a tool to convert text into numerical feature vectors.\n",
    "from nltk.corpus import stopwords # Imports the stopwords corpus from NLTK.\n",
    "import nltk # Imports the Natural Language Toolkit library.\n",
    "\n",
    "nltk.download('stopwords') # Downloads the NLTK stopwords list (if not already downloaded).\n",
    "\n",
    "# Defines a set of English stopwords. Using a set makes lookups very fast.\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Loads the training subset of the 20 Newsgroups dataset.\n",
    "# 'remove' strips common elements like headers, footers, and quotes to clean the text.\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Loads the test subset of the 20 Newsgroups dataset, applying the same cleaning.\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# --- Vectorizing with and without stopwords ---\n",
    "\n",
    "# Initializes a CountVectorizer without any stopword filtering.\n",
    "# This vectorizer will convert text into numerical counts of all words.\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Initializes a CountVectorizer configured to remove the specified 'stop_words'.\n",
    "# The 'stop_words' parameter expects a list, so the set is converted to a list.\n",
    "vectorizer_sw = CountVectorizer(stop_words=list(stop_words))\n",
    "\n",
    "# Transforms the training text data into a numerical feature matrix (X_train).\n",
    "# 'fit_transform' learns the vocabulary from the training data and then converts it.\n",
    "# This matrix will include all words.\n",
    "X_train = vectorizer.fit_transform(newsgroups_train.data)\n",
    "\n",
    "# Transforms the training text data into a numerical feature matrix (X_train_sw),\n",
    "# but this time, stopwords are excluded from the vocabulary.\n",
    "X_train_sw = vectorizer_sw.fit_transform(newsgroups_train.data)\n",
    "\n",
    "# Transforms the test text data using the vocabulary learned from 'vectorizer' (without stopwords removed during fitting).\n",
    "# 'transform' is used here because the vocabulary is already learned from the training data.\n",
    "X_test = vectorizer.transform(newsgroups_test.data)\n",
    "\n",
    "# Transforms the test text data using the vocabulary learned from 'vectorizer_sw' (with stopwords removed during fitting).\n",
    "# This ensures consistency in feature representation between training and testing sets.\n",
    "X_test_sw = vectorizer_sw.transform(newsgroups_test.data)\n",
    "\n",
    "# Extracts the numerical target labels (categories) for the training data.\n",
    "y_train = newsgroups_train.target\n",
    "\n",
    "# Extracts the numerical target labels (categories) for the test data.\n",
    "y_test = newsgroups_test.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af8d4ed",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We'll now execute machine learning algorithms to **classify text based on the 20 Newsgroups categories**. To assess performance with and without stopword removal, we'll employ **three different classification algorithms**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "075315c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes (Without stopwords removal) - Accuracy: 0.5431, F1: 0.5121\n",
      "Naive Bayes (With stopwords removal) - Accuracy: 0.6288, F1: 0.5907\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Felipe\\Documents\\Filipe docs\\ai projects\\algoritmos-aprendizado-maquina\\.venv\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM (Without stopwords removal) - Accuracy: 0.5720, F1: 0.5637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Felipe\\Documents\\Filipe docs\\ai projects\\algoritmos-aprendizado-maquina\\.venv\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM (With stopwords removal) - Accuracy: 0.5786, F1: 0.5690\n",
      "\n",
      "Random Forest (Without stopwords removal) - Accuracy: 0.5932, F1: 0.5713\n",
      "Random Forest (With stopwords removal) - Accuracy: 0.6147, F1: 0.5969\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB # Imports the Multinomial Naive Bayes classifier, suitable for text classification with word counts.\n",
    "from sklearn.svm import LinearSVC # Imports Linear Support Vector Classification, a robust linear classifier.\n",
    "from sklearn.ensemble import RandomForestClassifier # Imports the Random Forest classifier, an ensemble method that builds multiple decision trees.\n",
    "from sklearn.metrics import f1_score, accuracy_score # Imports F1-score and accuracy score metrics for evaluating classification models.\n",
    "\n",
    "# Defines a dictionary of machine learning models to be evaluated.\n",
    "# Keys are user-friendly names for the models, and values are instantiated model objects.\n",
    "models = {\n",
    "    'Naive Bayes': MultinomialNB(),\n",
    "    'SVM': LinearSVC(),\n",
    "    'Random Forest': RandomForestClassifier()\n",
    "}\n",
    "\n",
    "# Loops through each model in the 'models' dictionary.\n",
    "# 'name' will be the string key (e.g., 'Naive Bayes'), and 'model' will be the model object itself.\n",
    "for name, model in models.items():\n",
    "    # --- Evaluation without stopword removal ---\n",
    "    # Trains the current model using the training data (X_train) that INCLUDES stopwords.\n",
    "    # y_train contains the true categories for the training data.\n",
    "    model.fit(X_train, y_train)\n",
    "    # Makes predictions on the test data (X_test), which also includes stopwords.\n",
    "    predictions = model.predict(X_test)\n",
    "    # Prints the model's performance metrics for the case WITHOUT stopword removal.\n",
    "    # Uses f-strings for formatted output:\n",
    "    #   - {name}: The name of the current model.\n",
    "    #   - accuracy_score(y_test, predictions): Calculates the overall accuracy.\n",
    "    #   - f1_score(y_test, predictions, average='macro'): Calculates the F1-score.\n",
    "    #     'macro' average calculates F1-score for each class independently and then takes the unweighted mean,\n",
    "    #     useful when you want to treat all classes equally regardless of their size.\n",
    "    #   - :.4f ensures the numbers are formatted to 4 decimal places.\n",
    "    print(f\"{name} (Without stopwords removal) - Accuracy: {accuracy_score(y_test, predictions):.4f}, F1: {f1_score(y_test, predictions, average='macro'):.4f}\")\n",
    "\n",
    "    # --- Evaluation with stopword removal ---\n",
    "    # Trains the current model using the training data (X_train_sw) that EXCLUDES stopwords.\n",
    "    model.fit(X_train_sw, y_train)\n",
    "    # Makes predictions on the test data (X_test_sw), which also has stopwords removed.\n",
    "    predictions_sw = model.predict(X_test_sw)\n",
    "    # Prints the model's performance metrics for the case WITH stopword removal.\n",
    "    print(f\"{name} (With stopwords removal) - Accuracy: {accuracy_score(y_test, predictions_sw):.4f}, F1: {f1_score(y_test, predictions_sw, average='macro'):.4f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
