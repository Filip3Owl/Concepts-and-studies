{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86822eb6",
   "metadata": {},
   "source": [
    "___\n",
    "# TF-IDF\n",
    "\n",
    "### Term Frequency-Inverse Document Frequency\n",
    "\n",
    "TF-IDF is a widely used technique in information retrieval and text mining. It's a **statistical** measure that assesses a term's importance within a document or across a set of documents. TF-IDF considers both the **term's frequency in a document (TF)** and its **rearity across the entire document collection (IDF)**. This results in a numerical score representing the term's **relevance** to a specific document or corpus.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce5cf2b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The **TF-IDF technique** has diverse applications across fields like **information retrieval, document clustering, text classification, and search engine optimization**. It enables **efficient document classification and retrieval** by identifying relevant documents based on the similarity between query terms and document terms. TF-IDF also helps in **discovering important keywords** and **extracting significant insights** from large textual datasets. Understanding TF-IDF's fundamentals is crucial for anyone working with text data, as it provides a baseline for advanced text mining and natural language processing tasks.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a589df",
   "metadata": {},
   "source": [
    "### Understanding **TF**\n",
    "\n",
    "It covers **Term Frequency (TF)**, measuring how often a term appears in a document, and **Inverse Document Frequency (IDF)**, which weights rarer, more informative terms across a document collection.\n",
    "\n",
    "Before TF-IDF, we review the foundational **Bag-of-Words (BoW)** model. BoW represents text as vectors of word counts, ignoring order. It's simple and fast but loses context and treats all words equally.\n",
    "\n",
    "**TF-IDF** builds on this by combining TF and IDF to assign higher relevance to terms that are frequent in a document but rare in the overall corpus. This helps identify **key terms**.\n",
    "\n",
    "TF-IDF is crucial for **information retrieval, text classification, document clustering, and keyword discovery**, providing a strong numerical basis for advanced NLP tasks.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55c20d9",
   "metadata": {},
   "source": [
    "\n",
    "**Term Frequency (TF) scores** help determine the **relative importance of terms within a document**. In our example, \"football\" has the highest TF score, indicating it's the most significant term by frequency in that specific document. Understanding TF values is crucial for identifying a document's **dominant themes and topics**, and it forms half of the overall TF-IDF score calculation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fa7404",
   "metadata": {},
   "source": [
    "## UNDERSTANDING IDF\n",
    "\n",
    "\n",
    "### Summary: Understanding Inverse Document Frequency (IDF)\n",
    "\n",
    "**Inverse Document Frequency (IDF)** is another essential component of the TF-IDF framework. It measures a term's **rarity or uniqueness** across a collection of documents. IDF assigns a **higher weight to terms that appear less frequently** throughout the document set, as these terms are considered more informative and valuable for distinguishing between documents.\n",
    "\n",
    "The formula for IDF of a term '$t$' is the logarithm of the total number of documents in the corpus divided by the number of documents containing term '$t$'. If a term occurs in every document, its IDF will be 0 (since $\\log(1)=0$), meaning it's not useful for differentiation. Conversely, if a term appears in only a few documents (e.g., \"clustering\" appearing once in 100 documents), its IDF will be higher, indicating greater importance and differentiation potential.\n",
    "\n",
    "For example, if \"machine learning\" appears in 10 out of 100 documents, its IDF is $\\log(100/10) = \\log(10) = 1$. If \"clustering\" appears in only 1 out of 100 documents, its IDF is $\\log(100/1) = \\log(100) = 2$.\n",
    "\n",
    "In essence, IDF complements TF by highlighting a term's global importance. It's a crucial step in the TF-IDF framework, providing a more comprehensive assessment of term importance in text mining and information retrieval tasks.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d85176a",
   "metadata": {},
   "source": [
    "## Combining TF and IDF\n",
    "\n",
    "\n",
    "The **TF-IDF formula** merges the concepts of **Term Frequency (TF)** and **Inverse Document Frequency (IDF)**. It combines a term's local importance within a document (TF) with its global relevance across a document collection (IDF). This yields a weighted numerical score reflecting a term's overall significance in a specific document relative to the entire corpus.\n",
    "\n",
    "The formula is generally expressed as the **multiplication of TF(t, d) and IDF(t)**.\n",
    "\n",
    "* **TF(t, d)**: Represents the **Term Frequency** of term '$t$' in document '$d$', which is simply how many times '$t$' appears in '$d$'.\n",
    "* **IDF(t)**: Represents the **Inverse Document Frequency** of term '$t$'. It's calculated as the logarithm of the ratio between the total number of documents in the collection and the number of documents containing term '$t$'.\n",
    "\n",
    "By multiplying these values, the TF-IDF formula assigns higher scores to terms that are both **frequent within a particular document** and **rare across the entire document collection**. This method effectively identifies terms that are both locally significant and globally informative.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
