{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93b3de8f",
   "metadata": {},
   "source": [
    "# Implementing TF-IDF in Python with nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc3c514",
   "metadata": {},
   "source": [
    "To implement **TF-IDF in Python**, you typically follow a few core steps. First, you need to **preprocess your text documents**, which includes essential techniques like tokenization, stopword removal, and stemming. After preprocessing, you can calculate the TF-IDF scores using `TfidfVectorizer` from `sklearn.feature_extraction.text`. This class efficiently transforms your documents into **TF-IDF feature vectors**, which are then ready for subsequent text analysis tasks such as classification or clustering.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e55e2efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document: I love to play soccer\n",
      "  love: 0.5385\n",
      "  play: 0.5385\n",
      "  soccer: 0.3606\n",
      "  to: 0.5385\n",
      "\n",
      "Document: Soccer is my favorite sport\n",
      "  favorite: 0.5422\n",
      "  is: 0.4375\n",
      "  my: 0.4375\n",
      "  soccer: 0.3631\n",
      "  sport: 0.4375\n",
      "\n",
      "Document: I enjoy playing soccer with my friends\n",
      "  enjoy: 0.4428\n",
      "  friends: 0.4428\n",
      "  my: 0.3573\n",
      "  playing: 0.4428\n",
      "  soccer: 0.2966\n",
      "  with: 0.4428\n",
      "\n",
      "Document: Football is another popular sport\n",
      "  another: 0.4821\n",
      "  football: 0.4821\n",
      "  is: 0.3890\n",
      "  popular: 0.4821\n",
      "  sport: 0.3890\n",
      "\n",
      "Document: I don't like basketball\n",
      "  basketball: 0.5774\n",
      "  don: 0.5774\n",
      "  like: 0.5774\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk # Imports the Natural Language Toolkit library.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # Imports TfidfVectorizer for converting text to TF-IDF features.\n",
    "\n",
    "# Sample documents for demonstration.\n",
    "sample_documents = [\n",
    "    \"I love to play soccer\",\n",
    "    \"Soccer is my favorite sport\",\n",
    "    \"I enjoy playing soccer with my friends\",\n",
    "    \"Football is another popular sport\",\n",
    "    \"I don't like basketball\"\n",
    "]\n",
    "\n",
    "# Create the TF-IDF vectorizer object.\n",
    "# This object will learn the vocabulary and IDF values from the documents,\n",
    "# and then transform the documents into TF-IDF numerical representations.\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Compute the TF-IDF scores for the sample documents.\n",
    "# 'fit_transform' first learns the vocabulary and IDF values from 'sample_documents',\n",
    "# then transforms these documents into a sparse matrix of TF-IDF scores.\n",
    "tfidf_scores_matrix = tfidf_vectorizer.fit_transform(sample_documents)\n",
    "\n",
    "# Get the names of the features (terms) from the vectorizer's learned vocabulary.\n",
    "# These correspond to the columns in the TF-IDF matrix.\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Print the TF-IDF scores for each document.\n",
    "# 'enumerate' is used to get both the index (i) and the document content (doc).\n",
    "for i, doc in enumerate(sample_documents):\n",
    "    print(\"Document:\", doc) # Print the original document text.\n",
    "    # 'enumerate' is used again to get the index (j) and the term (feature_name).\n",
    "    for j, term in enumerate(feature_names):\n",
    "        # Access the TF-IDF score for the current document (i) and current term (j).\n",
    "        score = tfidf_scores_matrix[i, j]\n",
    "        # Only print terms that have a non-zero TF-IDF score in the current document.\n",
    "        if score > 0:\n",
    "            print(f\"  {term}: {score:.4f}\") # Format score to 4 decimal places for readability.\n",
    "    print() # Print an empty line for better separation between document outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2cb01a",
   "metadata": {},
   "source": [
    "---\n",
    "**TF-IDF scores** provide valuable insights into a term's importance within a document corpus. Understanding how to interpret these scores is key for various text mining and information retrieval tasks.\n",
    "\n",
    "**High TF-IDF scores** indicate a term is **frequent in a specific document** but **relatively rare across the entire corpus**. This suggests the term is highly **distinctive and significant** to that particular document's content.\n",
    "\n",
    "Conversely, **low TF-IDF scores** mean a term is **infrequent in a document** or **very common throughout the corpus**. These terms are typically less informative and contribute little to the unique understanding of a specific document (e.g., common words like \"the,\" \"and,\" or \"is\").\n",
    "\n",
    "Interpreting TF-IDF also involves **comparing scores** across different terms and documents. By examining scores within a single document, we can identify its most differentiating terms. Comparing scores across different documents helps pinpoint terms that are highly relevant or characteristic of specific documents or topics. This analysis is crucial for tasks like document clustering, topic modeling, and information retrieval, aiding in the identification and extraction of key textual information.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb83740",
   "metadata": {},
   "source": [
    "### TF-IDF: Information Retrieval's Core\n",
    "\n",
    "**TF-IDF** stands as a fundamental technique in **information retrieval (IR)**, pivotal for **ranking and retrieving relevant documents** in response to user queries. Search engines widely employ TF-IDF scores to effectively match query terms with document content, thereby delivering more precise search outcomes.\n",
    "\n",
    "Its primary applications within IR include:\n",
    "\n",
    "* **Document Ranking:** TF-IDF is instrumental in assessing a document's relevance to a given query. Documents with higher TF-IDF scores for the query's terms are prioritized and ranked higher in search results, ensuring users access the most pertinent information.\n",
    "* **Keyword Extraction:** The technique is highly effective at identifying **key terms or phrases** within documents by pinpointing those with elevated TF-IDF scores. These distinctive words are crucial assets for tasks like document indexing, categorization, and topic labeling.\n",
    "\n",
    "Ultimately, the inherent adaptability of TF-IDF significantly enhances capabilities in document ranking, keyword extraction, and overall information retrieval efficiency.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2994689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\tWord: document, TF-IDF Score: 0.4698\n",
      "\tWord: first, TF-IDF Score: 0.5803\n",
      "\tWord: is, TF-IDF Score: 0.3841\n",
      "\tWord: the, TF-IDF Score: 0.3841\n",
      "\tWord: this, TF-IDF Score: 0.3841\n",
      "\n",
      "Document 2:\n",
      "\tWord: document, TF-IDF Score: 0.6876\n",
      "\tWord: is, TF-IDF Score: 0.2811\n",
      "\tWord: second, TF-IDF Score: 0.5386\n",
      "\tWord: the, TF-IDF Score: 0.2811\n",
      "\tWord: this, TF-IDF Score: 0.2811\n",
      "\n",
      "Document 3:\n",
      "\tWord: and, TF-IDF Score: 0.5958\n",
      "\tWord: is, TF-IDF Score: 0.3109\n",
      "\tWord: the, TF-IDF Score: 0.3109\n",
      "\tWord: third, TF-IDF Score: 0.5958\n",
      "\tWord: this, TF-IDF Score: 0.3109\n",
      "\n",
      "Document 4:\n",
      "\tWord: document, TF-IDF Score: 0.4698\n",
      "\tWord: first, TF-IDF Score: 0.5803\n",
      "\tWord: is, TF-IDF Score: 0.3841\n",
      "\tWord: the, TF-IDF Score: 0.3841\n",
      "\tWord: this, TF-IDF Score: 0.3841\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk # Imports the Natural Language Toolkit library.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # Imports TfidfVectorizer to convert text to TF-IDF features.\n",
    "\n",
    "# Sample documents for demonstration.a\n",
    "documents = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third.\",\n",
    "    \"This is the first document?\"\n",
    "]\n",
    "\n",
    "# Preprocess the documents.\n",
    "# Each document is tokenized (split into words) after being converted to lowercase.\n",
    "processed_corpus = [nltk.word_tokenize(document.lower()) for document in documents]\n",
    "# Convert the preprocessed documents (list of tokens) back into strings, joined by spaces.\n",
    "processed_corpus = [' '.join(doc_tokens) for doc_tokens in processed_corpus]\n",
    "\n",
    "# Create the TF-IDF vectorizer.\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Compute the TF-IDF scores.\n",
    "# 'fit_transform' learns the vocabulary and IDF values from the 'processed_corpus',\n",
    "# then transforms these documents into a sparse matrix of TF-IDF scores.\n",
    "tfidf_scores_matrix = tfidf_vectorizer.fit_transform(processed_corpus)\n",
    "\n",
    "# Get the names of the features (words) from the vectorizer's learned vocabulary.\n",
    "# These correspond to the columns in the TF-IDF matrix.\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Print the TF-IDF scores for each document.\n",
    "# 'enumerate' is used to get both the document index and its corresponding scores.\n",
    "# '.toarray()' converts the sparse matrix row to a dense NumPy array for easier iteration.\n",
    "for doc_index, doc_scores in enumerate(tfidf_scores_matrix.toarray()):\n",
    "    print(f\"Document {doc_index + 1}:\") # Print the document number.\n",
    "    # 'enumerate' is used again to get the word index and its score within the current document.\n",
    "    for word_index, score in enumerate(doc_scores):\n",
    "        # Only print terms that have a non-zero TF-IDF score in the current document.\n",
    "        if score > 0:\n",
    "            # Print the word (feature_name) and its TF-IDF score, formatted to 4 decimal places.\n",
    "            print(f\"\\tWord: {feature_names[word_index]}, TF-IDF Score: {score:.4f}\")\n",
    "    print() # Print an empty line for better separation between document outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f462581",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This example demonstrates how to calculate **TF-IDF scores** for document classification using Python. We start by importing necessary libraries like **NLTK** and **`TfidfVectorizer` from scikit-learn**.\n",
    "\n",
    "We then define a list of sample documents and **preprocess** them using NLTK's `word_tokenize` to split them into words after converting to lowercase. These preprocessed tokens are then joined back into strings for the `TfidfVectorizer`.\n",
    "\n",
    "Next, we create a `TfidfVectorizer` instance and use its `fit_transform` method on our processed documents to **compute the TF-IDF scores**. We also retrieve the **feature names (words)** learned by the vectorizer.\n",
    "\n",
    "Finally, the code iterates through each document and its calculated TF-IDF scores, printing only the **non-zero scores** to highlight the most relevant terms for each document. This provides a clear visualization of how TF-IDF identifies important words.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c89e7dd",
   "metadata": {},
   "source": [
    "## TF-IDF for keyword extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c30427d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Keywords and their TF-IDF Scores:\n",
      "Keyword: apple, TF-IDF Score: 0.4264\n",
      "Keyword: banana, TF-IDF Score: 0.4264\n",
      "Keyword: orange, TF-IDF Score: 0.4264\n",
      "Keyword: this, TF-IDF Score: 0.2132\n",
      "Keyword: is, TF-IDF Score: 0.2132\n"
     ]
    }
   ],
   "source": [
    "import nltk # Imports the Natural Language Toolkit library.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # Imports TfidfVectorizer for converting text to TF-IDF features.\n",
    "\n",
    "# Sample document for keyword extraction.\n",
    "sample_document = \"This is a sample document that contains some keywords like apple, apple, banana, banana, and orange, orange.\"\n",
    "\n",
    "# Preprocess the document.\n",
    "# Convert the document to lowercase and then tokenize it (split into words).\n",
    "tokens = nltk.word_tokenize(sample_document.lower())\n",
    "\n",
    "# Create the TF-IDF vectorizer.\n",
    "# Note: TfidfVectorizer expects a list of strings as input, so we pass the tokenized\n",
    "# document rejoined into a single string within a list.\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Calculate the TF-IDF scores.\n",
    "# 'fit_transform' learns the vocabulary from the document and computes the TF-IDF scores.\n",
    "# Since we have only one document, the corpus for fit_transform is a list containing that single document.\n",
    "tfidf_scores_matrix = tfidf_vectorizer.fit_transform([' '.join(tokens)])\n",
    "\n",
    "# Get the names of the features (words) from the vectorizer's learned vocabulary.\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create a dictionary to store keywords and their TF-IDF scores.\n",
    "keywords_with_scores = {}\n",
    "\n",
    "# Get TF-IDF scores for each word in the document.\n",
    "# tfidf_scores_matrix is a sparse matrix. tfidf_scores_matrix.indices gives the column indices\n",
    "# of non-zero elements, and tfidf_scores_matrix.data gives their corresponding values.\n",
    "# Since we have only one document, we access the first (and only) row's data.\n",
    "for word_index, score in zip(tfidf_scores_matrix.indices, tfidf_scores_matrix.data):\n",
    "    word = feature_names[word_index]\n",
    "    # Store the word and its TF-IDF score.\n",
    "    # If a word appears multiple times, its score will be the same TF-IDF score\n",
    "    # calculated for that word in the document. We store it once.\n",
    "    if word not in keywords_with_scores: # This check is actually redundant here as each index appears once,\n",
    "                                      # but it's good practice for general scenarios.\n",
    "        keywords_with_scores[word] = score\n",
    "\n",
    "# Sort the keywords based on their TF-IDF scores in descending order.\n",
    "# 'items()' returns key-value pairs, 'key=lambda x: x[1]' sorts by the score (value),\n",
    "# and 'reverse=True' ensures descending order (highest score first).\n",
    "sorted_keywords = sorted(keywords_with_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the top 5 keywords and their TF-IDF scores.\n",
    "print(\"Top 5 Keywords and their TF-IDF Scores:\")\n",
    "for keyword, score in sorted_keywords[:5]:\n",
    "    print(f\"Keyword: {keyword}, TF-IDF Score: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0416e9c1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This example illustrates how to use **NLTK** and **scikit-learn** to calculate TF-IDF scores for **keyword extraction**.\n",
    "\n",
    "We begin by importing the necessary libraries and defining a **sample document**. The document is then **preprocessed** using NLTK's `word_tokenize` to convert it into tokens. An instance of `TfidfVectorizer` is created to handle the TF-IDF calculations.\n",
    "\n",
    "The `fit_transform` method of the `TfidfVectorizer` is called on our processed document to **compute the TF-IDF scores**, and the **feature names (words)** are retrieved. These scores, along with their corresponding words, are then stored in a dictionary.\n",
    "\n",
    "Finally, the keywords are **sorted by their TF-IDF scores** in descending order, and the top keywords are printed, showcasing the most relevant terms in the document.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2ae311",
   "metadata": {},
   "source": [
    "## TF-IDF for text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d8f59b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I do not like this product, Predicted label: positive\n"
     ]
    }
   ],
   "source": [
    "import nltk # Imports the Natural Language Toolkit library.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # Imports TfidfVectorizer to convert text into TF-IDF features.\n",
    "from sklearn.model_selection import train_test_split # Imports train_test_split for splitting data into training and testing sets.\n",
    "from sklearn.svm import LinearSVC # Imports Linear Support Vector Classification, a common classifier for text data.\n",
    "\n",
    "# Sample dataset\n",
    "# Each tuple contains a document text and its corresponding label (e.g., \"positive\", \"negative\").\n",
    "documents = [\n",
    "    (\"This is a positive review\", \"positive\"),\n",
    "    (\"I do not like this product\", \"negative\"),\n",
    "    (\"This movie is fantastic\", \"positive\"),\n",
    "    (\"The service was terrible\", \"negative\"),\n",
    "]\n",
    "\n",
    "# Preprocess the documents\n",
    "# Separate the text content (corpus) from their labels.\n",
    "corpus = [doc[0] for doc in documents] # Extracts all document texts.\n",
    "labels = [doc[1] for doc in documents] # Extracts all corresponding labels.\n",
    "\n",
    "# Create the TF-IDF vectorizer\n",
    "# This object will convert the text documents into numerical TF-IDF feature vectors.\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Split the dataset into training and testing sets.\n",
    "# test_size=0.2 means 20% of the data will be used for testing, 80% for training.\n",
    "# random_state=42 ensures reproducibility of the split.\n",
    "X_train, X_test, y_train, y_test = train_test_split(corpus, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Calculate TF-IDF features for the training set.\n",
    "# 'fit_transform' learns the vocabulary and IDF weights from the training data,\n",
    "# then transforms the training texts into TF-IDF vectors.\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test set using the *learned* vocabulary and IDF weights from the training set.\n",
    "# 'transform' is used here (not 'fit_transform') to avoid data leakage from the test set.\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Train a classifier (e.g., LinearSVC).\n",
    "# LinearSVC is a type of Support Vector Machine suitable for text classification.\n",
    "classifier = LinearSVC()\n",
    "classifier.fit(X_train_tfidf, y_train) # Train the classifier using the TF-IDF features and labels from the training set.\n",
    "\n",
    "# Predict categories for the test set.\n",
    "# The trained classifier makes predictions on the unseen test data.\n",
    "predictions = classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Print the predictions.\n",
    "# Iterate through the original test texts and their predicted labels.\n",
    "for i, prediction in enumerate(predictions):\n",
    "    print(f\"Text: {X_test[i]}, Predicted label: {prediction}\")\n",
    "\n",
    "# Note: Since the dataset is very small, the SVC classifier might make errors\n",
    "# or the split might result in a test set with very few or no examples for some classes,\n",
    "# affecting performance. This code is primarily for demonstration purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04aa62b7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This example demonstrates a basic **text classification pipeline** using TF-IDF and a LinearSVC classifier.\n",
    "\n",
    "We start by importing essential libraries: **NLTK**, `TfidfVectorizer`, and `LinearSVC` from scikit-learn. A small **sample dataset** of text documents with corresponding positive or negative labels is defined.\n",
    "\n",
    "We then prepare the data by creating a `TfidfVectorizer` instance and splitting the dataset into **training and testing sets** using `train_test_split`. TF-IDF features are calculated for the training set using `fit_transform`, and then *transformed* for the test set.\n",
    "\n",
    "Finally, a **LinearSVC classifier is trained** on the TF-IDF features of the training data. The trained classifier then **predicts labels for the test set**, and these predictions are printed. This showcases a complete, albeit simple, workflow for text classification.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e29b3b3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Summary: Challenges and Considerations for TF-IDF\n",
    "\n",
    "While **TF-IDF** is a widely used technique for text analysis, it comes with several challenges and considerations for researchers and practitioners. These often stem from the nature of the TF-IDF approach and the characteristics of the text data being analyzed.\n",
    "\n",
    "#### Document Length Bias\n",
    "\n",
    "One significant challenge is the **bias towards longer documents**. Since term frequency (TF) is directly proportional to document length, longer documents tend to have higher term frequencies, potentially skewing TF-IDF scores.\n",
    "\n",
    "To counter this, **normalization techniques** are applied:\n",
    "\n",
    "* **Simple Normalization:** Dividing the term's frequency by the total document length. For example, if \"analysis\" appears 10 times in a 100-word document (TF = 0.1) and 20 times in a 200-word document (TF = 0.1), normalization reveals the term has the same *proportional* importance, regardless of the document's size.\n",
    "* **Sublinear Term Frequency Scaling:** Applying a logarithmic transformation to the term frequency (e.g., $1 + \\log(\\text{term frequency})$). This helps reduce the impact of very high frequencies in longer documents, making terms like \"analysis\" (10 occurrences $\\rightarrow$ $1 + \\log(10) = 2$; 20 occurrences $\\rightarrow$ $1 + \\log(20) \\approx 2.3$) appear with more comparable importance.\n",
    "\n",
    "The choice of normalization method should align with the dataset's characteristics and analysis objectives.\n",
    "\n",
    "#### Stopwords and Rare Terms\n",
    "\n",
    "Another challenge is handling **stopwords** (common words like \"the,\" \"and,\" \"is\") which have high frequencies but low informational value. It's standard practice to **remove stopwords** to focus on more meaningful terms. Conversely, **rare terms** with very low frequencies might not provide significant insights and can introduce noise, so they are often filtered out.\n",
    "\n",
    "#### Vocabulary Size\n",
    "\n",
    "The **size of the vocabulary** (number of unique terms) in a document collection directly impacts the computational complexity and memory requirements for TF-IDF calculations. Large vocabularies can lead to increased processing time and memory usage. To mitigate this, **feature selection techniques** can be employed to select a subset of the most informative terms or to reduce the dimensionality of the TF-IDF matrix, improving efficiency.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dd88f6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "When working with TF-IDF, the size of the vocabulary can lead to high-dimensional matrices, impacting computational efficiency. A common **feature selection technique** to address this is **Variance Threshold**.\n",
    "\n",
    "This method aims to **remove terms with low variance** across documents in the TF-IDF matrix. The rationale is that terms whose TF-IDF scores show little variation between documents are not very useful for distinguishing one document from another and thus can be considered less informative.\n",
    "\n",
    "For instance, if you have a TF-IDF matrix of 1000 terms across 100 documents, you would calculate the variance of each term's TF-IDF score across all documents. By setting a **variance threshold** (e.g., 0.01), any term with a variance below this limit is removed. If the term \"analysis\" has a variance of 0.005 (below 0.01), it would be eliminated, thereby **reducing the matrix's dimensionality** and focusing on terms that truly differentiate content.\n",
    "\n",
    "While Variance Threshold is a straightforward way to improve TF-IDF efficiency, other feature selection techniques like SelectKBest, Chi-squared, and Mutual Information are also available, each with distinct criteria for selecting the most informative terms.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4977bea",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### TF-IDF Effectiveness in Diverse Scenarios\n",
    "\n",
    "The effectiveness of **TF-IDF** as a text analysis technique varies significantly based on the characteristics of the data and the specific task at hand.\n",
    "\n",
    "In **document classification**, TF-IDF has proven highly effective in identifying important features and improving classification accuracy. By assigning higher weights to rare terms that are particularly discriminative for a given class, TF-IDF helps differentiate between categories and enhances model performance. Researchers have consistently demonstrated TF-IDF's ability to boost the performance of classification models across various text classification tasks, including **sentiment analysis, topic categorization, and spam detection.**\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
