{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de027bfe",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "\n",
    "Word2Vec is a widely used technique for learning **vector representations of words** from large text datasets. Introduced by Mikolov et al. (2013), it has become a cornerstone in Natural Language Processing (NLP) for word representation.\n",
    "\n",
    "Word2Vec operates using two primary architectures:\n",
    "* **Continuous Bag of Words (CBOW)**\n",
    "* **Skip-Gram**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d438831e",
   "metadata": {},
   "source": [
    "\n",
    "| Feature               | Continuous Bag of Words (CBOW)                                     | Skip-Gram                                                        |\n",
    "| :-------------------- | :----------------------------------------------------------------- | :---------------------------------------------------------------- |\n",
    "| **Objective** | Predict a **target word** from its surrounding context words.      | Predict **context words** from a given target word.             |\n",
    "| **How it Works** | Given a window of context words, it tries to predict the central word. | Given a central word, it tries to predict the surrounding context words. |\n",
    "| **Example** | For \"The cat is in the ___\", with \"cat\" as target and \"The\", \"is\", \"in\", \"the\" as context, CBOW predicts \"cat\". | For central word \"cat\", Skip-Gram predicts context words like \"The\", \"is\", \"in\", \"the\". |\n",
    "| **Training Speed** | Generally **faster** to train.                                     | Generally **slower** to train.                                  |\n",
    "| **Quality of Embeddings** | Good, especially on smaller datasets.                               | Often produces **higher quality** word representations, especially with large datasets. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c066fd",
   "metadata": {},
   "source": [
    "According to Almeida and Xexéo (2019), the development of **prediction-based models for word representations (embeddings)** is intrinsically linked to the history of **Neural Network Language Models (NNLMs)**, as embeddings were initially conceived as the raw vector projection in the first \"representation layer\" of these models.\n",
    "\n",
    "The history of NNLMs has primarily been one of **gradual efficiency gains**, occasional insights, and trade-offs between model complexity and the ability to train with larger datasets. Early results showed NNLMs outperformed n-gram based predecessors in language modeling, but **long training times (days to weeks)** were a significant hurdle.\n",
    "\n",
    "Mikolov et al. (2013) noted that while methods like LSA and LDA existed for creating continuous word representations, their Word2Vec studies focused on **distributed word representations learned by neural networks**. These neural network-based embeddings demonstrated superior performance in preserving linear relationships between words compared to LSA. Furthermore, LDA proved computationally expensive for large datasets. To evaluate model architectures, Mikolov et al. (2013) introduced a **computational complexity metric** measuring the number of parameters accessed during training. The goal was to maximize model accuracy while minimizing computational complexity, striking a balance between performance and efficiency in word representation.\n",
    "\n",
    "The \"nonlinear\" models proposed by Mikolov et al. (2013) were two new architectures designed to learn distributed word representations with reduced computational complexity. A key observation was that most of the complexity stemmed from the nonlinear hidden layer typical of neural networks. While this nonlinearity is a strength of neural networks, the researchers opted to explore **simpler models**. Although these simpler models might not represent data as precisely as full neural networks, they could be trained **much more efficiently on significantly larger datasets**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f478b47",
   "metadata": {},
   "source": [
    "| Concept        | Latent Semantic Analysis (LSA)                                     | Latent Dirichlet Allocation (LDA)                                |\n",
    "| :------------- | :----------------------------------------------------------------- | :--------------------------------------------------------------- |\n",
    "| **Type** | Statistical method (matrix factorization)                          | Generative probabilistic model                                   |\n",
    "| **Objective** | Discover relationships between terms and documents by identifying \"latent concepts\" (topics). | Discover abstract \"topics\" within a collection of documents and assign topics to documents. |\n",
    "| **How it Works** | Builds a term-document matrix (word frequencies in documents) and applies **Singular Value Decomposition (SVD)** to reduce dimensionality and reveal underlying semantic structures. | Assumes each document is a **mixture of topics**, and each topic is a **mixture of words**. Infers these underlying distributions. |\n",
    "| **Core Idea** | Words that appear in similar contexts have similar meanings. Uncovers latent semantic relationships by representing words and documents in a lower-dimensional \"concept space\". | Documents are composed of a distribution of topics, and topics are characterized by a distribution of words. Aims to reverse-engineer this generative process. |\n",
    "| **Strengths** | - Effective for dimensionality reduction.\\<br\\>- Can find synonyms and related terms.\\<br\\>- Computationally less expensive than some neural models for certain tasks. | - Probabilistic, offering more interpretable topic mixtures.\\<br\\>- Handles polysemy (words with multiple meanings) better than LSA.\\<br\\>- Can assign multiple topics to a single document. |\n",
    "| **Limitations** | - Does not account for word order or context.\\<br\\>- Struggles with polysemy (averages out meanings of a word).\\<br\\>- Assumes linear relationships. | - Requires specifying the number of topics beforehand.\\<br\\>- Computationally more intensive than LSA for very large datasets (though efficient for many topic modeling tasks). |\n",
    "| **Primary Use** | Information retrieval, document clustering, early topic modeling.   | Topic modeling, document classification, recommender systems.     |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1fb98f",
   "metadata": {},
   "source": [
    "A proposed architecture, similar to a feedforward NNLM, removes the nonlinear hidden layer and shares the projection layer across all words, not just the projection matrix. This means all words are projected to the same position, and their vectors are averaged. This model is called a bag-of-words model because word order doesn't affect the projection.\n",
    "\n",
    "The model also incorporates future words. The best performance on the described task was achieved using a log-linear classifier with four past and four future words as input. The training objective is to correctly classify the current (middle) word.\n",
    "\n",
    "The training complexity is represented by the formula:\n",
    "\n",
    "Q = N × D + D × log2(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02714d3",
   "metadata": {},
   "source": [
    "Imagine you're trying to guess the **middle word** in a sentence.\n",
    "\n",
    "This new method is like a super-simple way to do that. Instead of a complicated process, it just **averages out** the meaning of all the words around the middle word (both before and after it). It doesn't care about the order of these surrounding words, which is why it's called a \"**bag-of-words**\" model – like all the words are just thrown into a bag.\n",
    "\n",
    "Then, it uses this average meaning to try and figure out what the middle word should be.\n",
    "\n",
    "The formula you see ($Q = N \\times D + D \\times \\log_2(V)$) is just a way to measure **how much \"work\"** the computer has to do to learn from all the words.\n",
    "* $N$ is like the total number of words it's looking at.\n",
    "* $D$ is how complex the \"meaning\" of each word is (like how many details it pays attention to).\n",
    "* $V$ is the total number of unique words it knows.\n",
    "\n",
    "So, in short, it's a straightforward way to predict a word by simply combining the meanings of its neighbors, and the formula tells you how much computing power it needs to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1058744e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Examples of context and target words\n",
    "examples = [\n",
    "    ([\"The\", \"fly\"], \"birds\"),\n",
    "    ([\"birds\", \"gracefully\"], \"fly\"),\n",
    "    ([\"fly\", \"through\"], \"gracefully\"),\n",
    "    ([\"gracefully\", \"sky\"], \"through\")\n",
    "]\n",
    "\n",
    "# Load a pre-trained Word2Vec model (e.g., the \"word2vec-google-news-300\" model)\n",
    "# Make sure you have the pre-trained model downloaded and the correct path provided\n",
    "# Note: 'load_word2vec_format' is deprecated. Consider using 'load' for models saved with model.save()\n",
    "# or adjust based on your model's saving format.\n",
    "model_w2v = Word2Vec.load_word2vec_format('path/to/your/model.bin', binary=True)\n",
    "\n",
    "# Iterate over the examples and print the similarities\n",
    "for context, target_word in examples:\n",
    "    similarity = model_w2v.wv.n_similarity(context, [target_word])\n",
    "    print(f\"Similarity between {context} and {target_word}: {similarity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b732385f",
   "metadata": {},
   "source": [
    "\n",
    "## SKIP-GRAM Architecture Explained\n",
    "\n",
    "The **SKIP-GRAM** architecture takes a different approach than predicting a word from its context. Instead, it aims to **predict surrounding words based on a given \"current\" word**. Think of it as the reverse of the previous model.\n",
    "\n",
    "Here's how it works: for every word in a sentence, the model uses that word as input to a simple log-linear classifier. Its goal is to correctly predict words that fall within a certain range (both before and after) the input word. The wider this range, the better the quality of the resulting word vectors, but it also increases the computational cost.\n",
    "\n",
    "### Training Complexity\n",
    "\n",
    "The training complexity for this architecture is directly proportional to:\n",
    "\n",
    "$Q = C \\times (D + D \\times \\log_2(V))$\n",
    "\n",
    "Where:\n",
    "* $C$ is the **maximum distance** between the words.\n",
    "* $D$ represents the **dimensionality of the word vectors** (how much \"information\" each word's representation holds).\n",
    "* $V$ is the **size of the vocabulary** (the total number of unique words the model knows).\n",
    "\n",
    "For example, if you choose a maximum distance $C = 5$, for each training word, the model randomly selects a number $R$ between 1 and 5. It then uses $R$ words from the past and $R$ words from the future of the current word as the correct labels. This means for each current word, the model performs $R \\times 2$ classifications, with the current word as input and each of the $R + R$ surrounding words as outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5155740b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "\n",
    "# Load your training data as a list of lists of words.\n",
    "# Each inner list represents a sentence.\n",
    "# Here, I'll create a simple example with fictional sentences.\n",
    "sentences = [\n",
    "    [\"the\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"],\n",
    "    [\"she\", \"sells\", \"seashells\", \"by\", \"the\", \"seashore\"],\n",
    "]\n",
    "\n",
    "# Train the CBOW model (Continuous Bag-of-Words)\n",
    "cbow_model = Word2Vec(sentences, sg=0, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Train the Skip-Gram model\n",
    "skipgram_model = Word2Vec(sentences, sg=1, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Now, you can use the models to get word vector representations.\n",
    "# For example, to get the vector for the word \"fox\" in the CBOW model:\n",
    "vector_cbow_fox = cbow_model.wv['fox']\n",
    "\n",
    "# To get the vector for the word \"fox\" in the Skip-Gram model:\n",
    "vector_skipgram_fox = skipgram_model.wv['fox']\n",
    "\n",
    "# To calculate the similarity between two words, you can do the following:\n",
    "similarity = cbow_model.wv.similarity('fox', 'dog')\n",
    "\n",
    "# This returns a similarity value between -1 and 1.\n",
    "print(f\"Similarity between 'fox' and 'dog' (CBOW): {similarity}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
