{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa59516b",
   "metadata": {},
   "source": [
    "# POS-TAGGING APLICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1bbbaf",
   "metadata": {},
   "source": [
    "**Part-of-Speech (POS) Tagging** is a fundamental task in Natural Language Processing (NLP) that involves **assigning a grammatical category (or \"tag\") to each word in a given text.**\n",
    "\n",
    "**Objective:** To identify the lexical category of each word, such as noun, verb, adjective, adverb, pronoun, preposition, conjunction, interjection, etc., based on its definition and its context within the sentence.\n",
    "\n",
    "**How it Works:**\n",
    "\n",
    "* **Input:** A sequence of words (a sentence or a chunk of text).\n",
    "* **Output:** A sequence of words, where each word is paired with its corresponding POS tag.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "* **Sentence:** \"The quick brown fox jumps over the lazy dog.\"\n",
    "* **POS Tagged Output:**\n",
    "    * \"The\": Determiner (DT)\n",
    "    * \"quick\": Adjective (JJ)\n",
    "    * \"brown\": Adjective (JJ)\n",
    "    * \"fox\": Noun (NN)\n",
    "    * \"jumps\": Verb (VBZ)\n",
    "    * \"over\": Preposition (IN)\n",
    "    * \"the\": Determiner (DT)\n",
    "    * \"lazy\": Adjective (JJ)\n",
    "    * \"dog\": Noun (NN)\n",
    "\n",
    "**Importance/Applications:**\n",
    "\n",
    "* **Foundation for Higher-Level NLP Tasks:** POS tagging is a crucial preprocessing step for many more complex NLP applications.\n",
    "* **Word Sense Disambiguation:** Helps to understand the correct meaning of a word that might have multiple senses (e.g., \"bank\" as a financial institution vs. \"bank\" as a river bank).\n",
    "* **Syntactic Parsing:** Essential for building parse trees and understanding the grammatical structure of sentences.\n",
    "* **Named Entity Recognition (NER):** Helps to identify proper nouns, locations, organizations, etc.\n",
    "* **Machine Translation:** Provides grammatical information that can guide translation.\n",
    "* **Information Extraction:** Aids in extracting specific data from text.\n",
    "* **Text-to-Speech Systems:** Helps determine pronunciation and intonation (e.g., \"read\" - present vs. past tense).\n",
    "\n",
    "**Challenges:**\n",
    "\n",
    "* **Ambiguity:** Many words can function as different parts of speech depending on the context (e.g., \"book\" as a noun vs. \"book\" as a verb).\n",
    "* **New Words/Slang:** Models need to be robust enough to handle words not seen during training.\n",
    "\n",
    "**Common Approaches:**\n",
    "\n",
    "* **Rule-Based Tagging:** Uses hand-crafted rules based on suffixes, prefixes, and context.\n",
    "* **Stochastic/Statistical Tagging:** Uses probability based on how frequently a word appears with a certain tag and how frequently one tag follows another. (e.g., Hidden Markov Models - HMMs, Maximum Entropy Models).\n",
    "* **Neural Network-Based Tagging:** Uses deep learning models (like RNNs, LSTMs, Transformers) to learn complex patterns from data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c38afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Felipe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Felipe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('will', 'MD'),\n",
       " ('buy', 'VB'),\n",
       " ('ice', 'JJ'),\n",
       " ('cream', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk # Import the Natural Language Toolkit (NLTK) library, which is a powerful tool for working with human language data.\n",
    "\n",
    "# Download the 'averaged_perceptron_tagger' resource. This is a pre-trained statistical model\n",
    "# used by NLTK for Part-of-Speech (POS) tagging. It assigns grammatical categories (like noun, verb, adjective)\n",
    "# to words in a text. This resource needs to be downloaded once to be available for use.\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Download the 'punkt' tokenizer models. This resource contains pre-trained models for tokenizing\n",
    "# text into sentences and words. Tokenization is the process of breaking down a text into smaller units.\n",
    "# 'punkt' is essential for the `word_tokenize` function used below. This also needs to be downloaded once.\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Define a sample text string.\n",
    "text_sentence = \"I will buy ice cream.\"\n",
    "\n",
    "# Tokenize the text_sentence into individual words. The `nltk.word_tokenize()` function splits the string\n",
    "# into a list of words and punctuation marks.\n",
    "# For example, \"I will buy ice cream.\" becomes ['I', 'will', 'buy', 'ice', 'cream', '.'].\n",
    "words_tokenized = nltk.word_tokenize(text_sentence)\n",
    "\n",
    "# Perform Part-of-Speech (POS) tagging on the tokenized words.\n",
    "# The `nltk.pos_tag()` function takes a list of words and returns a list of tuples,\n",
    "# where each tuple contains a word and its corresponding POS tag.\n",
    "# For example, ['I', 'will', 'buy', 'ice', 'cream', '.'] might become\n",
    "# [('I', 'PRP'), ('will', 'MD'), ('buy', 'VB'), ('ice', 'NN'), ('cream', 'NN'), ('.', '.')]\n",
    "# (PRP: Personal Pronoun, MD: Modal Verb, VB: Verb Base Form, NN: Noun, .: Punctuation mark)\n",
    "pos_tagged_words = nltk.pos_tag(words_tokenized)\n",
    "\n",
    "# The result of `nltk.pos_tag(text)` will be a list of tuples, which would typically be printed\n",
    "# or used for further NLP tasks.\n",
    "print(pos_tagged_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d70708e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('O', 'DET'), ('mistério', 'NOUN'), ('foi', 'AUX'), ('resolvido', 'VERB'), ('antes', 'ADV'), ('do', 'PRON'), ('esperado', 'VERB'), ('!', 'PUNCT')]\n"
     ]
    }
   ],
   "source": [
    "#python -m spacy download pt_core_news_sm\n",
    "\n",
    "# Import the spaCy library, which is an open-source library for advanced Natural Language Processing (NLP) in Python.\n",
    "# spaCy is known for its efficiency, speed, and production-ready NLP models.\n",
    "import spacy \n",
    "\n",
    "# Load a pre-trained spaCy language model for Portuguese.\n",
    "# 'pt_core_news_sm' refers to a small (sm) Portuguese (pt) model trained on news text (core_news).\n",
    "# This model includes components for tokenization, POS tagging, dependency parsing, named entity recognition (NER), etc.\n",
    "# The loaded model object, assigned to the variable 'PLN', is essentially the \"pipeline\" for processing Portuguese text.\n",
    "nlp_pipeline_pt = spacy.load('pt_core_news_sm')\n",
    "\n",
    "# Process a given text string using the loaded spaCy model.\n",
    "# When a string is passed to the nlp_pipeline_pt object, spaCy processes it through its various components\n",
    "# (tokenizer, tagger, parser, etc.) and returns a 'Doc' object.\n",
    "# The 'Doc' object is a container for all the processed information about the text.\n",
    "text_to_process = \"O mistério foi resolvido antes do esperado!\"\n",
    "doc_object = nlp_pipeline_pt(text_to_process)\n",
    "\n",
    "# Iterate through each 'token' (word or punctuation mark) in the processed 'Doc' object.\n",
    "# For each token, extract its original text ('token.text') and its Part-of-Speech (POS) tag ('token.pos_').\n",
    "# The POS tag represents the grammatical category of the word (e.g., NOUN, VERB, ADJ, PRON).\n",
    "# The results are collected into a list of tuples, where each tuple is (word, POS_tag).\n",
    "# This provides a quick way to see the POS tagging performed by the spaCy model.\n",
    "pos_tagged_list = [(token.text, token.pos_) for token in doc_object]\n",
    "\n",
    "# The 'pos_tagged_list' would then contain something like:\n",
    "# [('They', 'PRON'), ('are', 'AUX'), ('solving', 'VERB'), ('a', 'DET'), ('mystery', 'NOUN'), ('!', 'PUNCT')]\n",
    "# This output would typically be printed or used for further analysis.\n",
    "print(pos_tagged_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2bf0f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-02 08:23:54 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19cb86ce4d004523af8a013a16350f95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-02 08:23:55 INFO: Downloaded file to C:\\Users\\Felipe\\stanza_resources\\resources.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "882313c3c290473bbd17007afb24ddc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-pt/resolve/v1.10.0/models/tokenize/bosque.pt:   0%|     …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "facc3607099e41ce9f4486983d5d4c44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-pt/resolve/v1.10.0/models/mwt/bosque.pt:   0%|          …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "352f819c0d6346cb93ebec4970ec0447",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-pt/resolve/v1.10.0/models/pos/bosque_charlm.pt:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08e7d67eae59400d97342c509a3bc97e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-pt/resolve/v1.10.0/models/lemma/bosque_nocharlm.pt:   0%…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c85c60e71ded4e00828a356c0e302c58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-pt/resolve/v1.10.0/models/constituency/cintil_charlm.pt:…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57114f2b47e0402da269e799682ba4a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-pt/resolve/v1.10.0/models/depparse/bosque_charlm.pt:   0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c293886d4cf04517bf1165e897f2fca9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-pt/resolve/v1.10.0/models/forward_charlm/oscar2023.pt:  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcdacc66071e402883f37217682bc18c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-pt/resolve/v1.10.0/models/backward_charlm/oscar2023.pt: …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a9fbde0eb3345b3b14f825ad5d1c45e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-pt/resolve/v1.10.0/models/pretrain/conll17.pt:   0%|    …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-02 08:25:50 INFO: Loading these models for language: pt (Portuguese):\n",
      "==================================\n",
      "| Processor    | Package         |\n",
      "----------------------------------\n",
      "| tokenize     | bosque          |\n",
      "| mwt          | bosque          |\n",
      "| pos          | bosque_charlm   |\n",
      "| lemma        | bosque_nocharlm |\n",
      "| constituency | cintil_charlm   |\n",
      "| depparse     | bosque_charlm   |\n",
      "==================================\n",
      "\n",
      "2025-06-02 08:25:50 INFO: Using device: cpu\n",
      "2025-06-02 08:25:50 INFO: Loading: tokenize\n",
      "2025-06-02 08:25:53 INFO: Loading: mwt\n",
      "2025-06-02 08:25:53 INFO: Loading: pos\n",
      "2025-06-02 08:25:58 INFO: Loading: lemma\n",
      "2025-06-02 08:25:59 INFO: Loading: constituency\n",
      "2025-06-02 08:26:00 INFO: Loading: depparse\n",
      "2025-06-02 08:26:01 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Existem', 'VERB'), ('muitas', 'DET'), ('possibilidades', 'NOUN'), ('.', 'PUNCT'), ('Porém', 'ADV'), (',', 'PUNCT'), ('não', 'ADV'), ('evidentes', 'ADJ'), ('.', 'PUNCT')]\n"
     ]
    }
   ],
   "source": [
    "from stanza import Pipeline\n",
    "\n",
    "PLN = Pipeline(lang='pt')\n",
    "\n",
    "doc = PLN(\"Existem muitas possibilidades. Porém, não evidentes.\")\n",
    "\n",
    "tags = [(word.text, word.upos) for sent in doc.sentences for word in sent.words]\n",
    "\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e4c419c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Só', 'NNP'),\n",
       " ('vai', 'NNP'),\n",
       " ('ser', 'NN'),\n",
       " ('resolvido', 'NN'),\n",
       " ('em', 'VBD'),\n",
       " ('20', 'CD'),\n",
       " ('anos', 'NNS')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "blob = TextBlob(\"Só vai ser resolvido em 20 anos.\")\n",
    "\n",
    "blob.tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb37027",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Felipe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Felipe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Information extraction\n",
    "\n",
    "import nltk\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Ensure the necessary NLTK resources are downloaded. These models are required for tokenization and POS tagging.\n",
    "nltk.download('punkt') # Downloads the 'punkt' tokenizer models, used by word_tokenize.\n",
    "nltk.download('averaged_perceptron_tagger') # Downloads the pre-trained POS tagger model.\n",
    "\n",
    "def extract_nouns(text):\n",
    "    \"\"\"\n",
    "    Extracts nouns from a given text using NLTK's word tokenization and Part-of-Speech (POS) tagging.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text from which to extract nouns.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of words identified as nouns.\n",
    "    \"\"\"\n",
    "    # Tokenize the input text into a list of words.\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    # Perform Part-of-Speech tagging on the tokenized words.\n",
    "    # This assigns a grammatical tag (e.g., 'NN', 'VB') to each word.\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "\n",
    "    # Define the POS tags corresponding to nouns in the Penn Treebank tagset.\n",
    "    # NN: Noun, singular or mass (e.g., 'table', 'water')\n",
    "    # NNS: Noun, plural (e.g., 'tables', 'waters')\n",
    "    # NNP: Proper noun, singular (e.g., 'John', 'London')\n",
    "    # NNPS: Proper noun, plural (e.g., 'Americans', 'Russians')\n",
    "    noun_tags = [\"NN\", \"NNS\", \"NNP\", \"NNPS\"]\n",
    "\n",
    "    # Filter the words based on their POS tags to include only nouns.\n",
    "    # [ <expressão de saída> for <item_iterado> in <iterável> if <condição_opcional> ]\n",
    "    nouns = [word for word, tag in tags if tag in noun_tags]\n",
    "    return nouns\n",
    "\n",
    "# Load the 20 Newsgroups dataset.\n",
    "# 'subset='all'' fetches both training and test sets.\n",
    "# 'remove=('headers', 'footers', 'quotes')' optionally cleans the text by removing common boilerplate\n",
    "# content, making the core content more prominent for analysis.\n",
    "newsgroups_data = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Select a sample text from the dataset.\n",
    "# newsgroups_data.data[10] accesses the 11th document (Python lists are 0-indexed).\n",
    "# .split(\"\\n\")[3] attempts to get the 4th line of that document.\n",
    "# This approach might result in an empty or less meaningful line depending on the document and cleaning.\n",
    "# It's more robust to select the entire document or check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f76e28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('want', 'go')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# (Assuming nltk.download('punkt') and nltk.download('averaged_perceptron_tagger') have been run previously\n",
    "# to ensure the necessary resources for tokenization and POS tagging are available.)\n",
    "\n",
    "def find_verb_verb_sequences(text):\n",
    "    \"\"\"\n",
    "    Identifies and returns sequences of two consecutive verbs in a given text.\n",
    "    This function can be a simple helper for grammatical correction, as consecutive verbs\n",
    "    (especially main verbs) often indicate a grammatical error in English (e.g., \"I want go\").\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text string to analyze.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tuples, where each tuple contains two consecutive words\n",
    "              that were identified as verbs.\n",
    "    \"\"\"\n",
    "    # 1. Tokenize the input text into individual words and punctuation marks.\n",
    "    # Example: \"I want go there.\" -> ['I', 'want', 'go', 'there', '.']\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    # 2. Perform Part-of-Speech (POS) tagging on the tokenized words.\n",
    "    # This assigns a grammatical tag (e.g., 'VB', 'NNP') to each word.\n",
    "    # Example: ['I', 'want', 'go', 'there', '.'] -> [('I', 'PRP'), ('want', 'VBP'), ('go', 'VB'), ('there', 'RB'), ('.', '.')]\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "\n",
    "    # 3. Identify consecutive verb sequences using a list comprehension.\n",
    "    # This comprehension iterates through the 'tags' list.\n",
    "    # `range(len(tags)-1)`: This ensures the loop goes up to the second-to-last element,\n",
    "    # because we are always looking at `tags[i]` and `tags[i+1]`.\n",
    "    # `tags[i][1].startswith(\"VB\")`: Checks if the POS tag of the current word (tags[i][1]) starts with \"VB\".\n",
    "    #    \"VB\" is the prefix for all verb tags in the Penn Treebank tagset (e.g., VB, VBD, VBG, VBN, VBP, VBZ).\n",
    "    # `tags[i+1][1].startswith(\"VB\")`: Checks if the POS tag of the *next* word (tags[i+1][1]) also starts with \"VB\".\n",
    "    # If both conditions are true, then:\n",
    "    # `(tags[i][0], tags[i+1][0])`: A tuple containing the actual words (tags[i][0] is the word for the current tag,\n",
    "    #    tags[i+1][0] is the word for the next tag) is added to the `verb_verb` list.\n",
    "    verb_verb = [(tags[i][0], tags[i+1][0]) #list comprehension\n",
    "                 for i in range(len(tags)-1)\n",
    "                 if tags[i][1].startswith(\"VB\") and tags[i+1][1].startswith(\"VB\")]\n",
    "\n",
    "    # 4. Return the list of identified consecutive verb sequences.\n",
    "    return verb_verb\n",
    "\n",
    "# Define a sample text string to test the function.\n",
    "sample_text = \"I want go there.\" # This sentence has a common grammatical error: \"want go\" (should be \"want to go\" or \"wanna go\").\n",
    "\n",
    "# Call the function with the sample text and print the result.\n",
    "# Expected Output: [('want', 'go')] because 'want' is a verb and 'go' is a verb.\n",
    "print(find_verb_verb_sequences(sample_text))\n",
    "\n",
    "# Another example:\n",
    "# sample_text_2 = \"They plan to eat pizza. We will finish soon. He likes swimming.\"\n",
    "# print(find_verb_verb_sequences(sample_text_2))\n",
    "# Expected output for sample_text_2 might be empty or specific modal-verb sequences depending on NLTK's tagging.\n",
    "# For \"He likes swimming.\", 'likes' (VBZ) and 'swimming' (VBG, acting as a noun/gerund here but tagged as verb-like)\n",
    "# could be flagged depending on the exact context and tagger's interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4fe220f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fruta\n",
      "parte da roupa\n"
     ]
    }
   ],
   "source": [
    "def disambiguate_word(word, context):\n",
    "    \"\"\"Attempts to disambiguate the meaning of a word based on its context.\n",
    "\n",
    "    This is a simplified example that uses a dictionary of ambiguous words and their\n",
    "    possible meanings with associated keywords.  It checks if any of the keywords\n",
    "    for a particular meaning are present in the context.  This is a very basic\n",
    "    Word Sense Disambiguation (WSD) approach and would not be robust in real-world scenarios.\n",
    "\n",
    "    Args:\n",
    "        word (str): The ambiguous word to disambiguate.\n",
    "        context (str): The surrounding text providing context for the word.\n",
    "\n",
    "    Returns:\n",
    "        str: The disambiguated meaning of the word, or None if the word is not in the dictionary\n",
    "             or no matching context is found.\n",
    "    \"\"\"\n",
    "    word_meanings = {\n",
    "        'manga': [\n",
    "            {'meaning': 'fruta', 'keywords': ['comer', 'doce', 'fruta', 'vitamina', 'café da manhã']},  # Added 'café da manhã' for contexto1\n",
    "            {'meaning': 'parte da roupa', 'keywords': ['vestir', 'camisa', 'tecido', 'costura']}\n",
    "        ],\n",
    "        # ... (outras palavras ambíguas podem ser adicionadas aqui)\n",
    "    }\n",
    "    if word not in word_meanings:\n",
    "        return None\n",
    "    meanings = word_meanings[word]\n",
    "    for meaning_data in meanings:\n",
    "        for keyword in meaning_data['keywords']:\n",
    "            if keyword in context:\n",
    "                return meaning_data['meaning']\n",
    "    return None\n",
    "\n",
    "contexto1 = \"Eu gosto de comer manga no café da manhã.\"\n",
    "contexto2 = \"A manga da minha camisa rasgou.\"\n",
    "print(disambiguate_word('manga', contexto1))  # Esperado: 'fruta'\n",
    "print(disambiguate_word('manga', contexto2))  # Esperado: 'parte da roupa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ac811195",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Felipe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'fly', 'be', 'fly', '.']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer # Import the WordNetLemmatizer for lemmatization.\n",
    "from nltk.corpus import wordnet # Import the WordNet corpus, which is a lexical database for English.\n",
    "\n",
    "# Download the 'wordnet' resource. This is necessary for the WordNetLemmatizer to function correctly,\n",
    "# as it uses WordNet to look up lemmas. This needs to be downloaded once.\n",
    "nltk.download('wordnet')\n",
    "# Ensure 'punkt' and 'averaged_perceptron_tagger' are also downloaded for tokenization and POS tagging.\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"\n",
    "    Converts a Penn Treebank POS tag to a WordNet POS tag.\n",
    "    The WordNetLemmatizer requires specific WordNet POS tags (NOUN, VERB, ADJ, ADV)\n",
    "    for accurate lemmatization, while nltk.pos_tag returns Penn Treebank tags.\n",
    "\n",
    "    Args:\n",
    "        treebank_tag (str): The POS tag returned by nltk.pos_tag (e.g., 'NN', 'VBP', 'JJ').\n",
    "\n",
    "    Returns:\n",
    "        str: The corresponding WordNet POS tag (e.g., wordnet.NOUN, wordnet.VERB).\n",
    "        Defaults to wordnet.NOUN if no specific mapping is found.\n",
    "    \"\"\"\n",
    "    if treebank_tag.startswith('J'): # 'J' for Adjective (e.g., JJ, JJR, JJS)\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'): # 'V' for Verb (e.g., VB, VBD, VBG, VBN, VBP, VBZ)\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'): # 'N' for Noun (e.g., NN, NNS, NNP, NNPS)\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'): # 'R' for Adverb (e.g., RB, RBR, RBS)\n",
    "        return wordnet.ADV\n",
    "    else: # Default to Noun if the tag is not recognized or doesn't start with a specific letter.\n",
    "        return wordnet.NOUN\n",
    "\n",
    "# Initialize the WordNet Lemmatizer.\n",
    "lemmatizer_instance = WordNetLemmatizer()\n",
    "\n",
    "# Define a sample text and tokenize it into words.\n",
    "sample_text = \"The flies are flying.\"\n",
    "text_tokens = nltk.word_tokenize(sample_text)\n",
    "\n",
    "# Perform Part-of-Speech tagging on the tokens.\n",
    "# This is crucial because lemmatization accuracy heavily depends on knowing the word's POS.\n",
    "pos_tags = nltk.pos_tag(text_tokens)\n",
    "\n",
    "# Lemmatize each word in the tokenized and tagged list.\n",
    "# For each (word, tag) pair:\n",
    "# 1. 'word' is the actual word.\n",
    "# 2. 'get_wordnet_pos(tag)' converts the NLTK POS tag to a WordNet-compatible POS tag.\n",
    "# 3. 'lemmatizer_instance.lemmatize()' then finds the base form (lemma) of the word,\n",
    "#    using the provided WordNet POS tag for context.\n",
    "lemmatized_words_list = [lemmatizer_instance.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tags]\n",
    "\n",
    "# Print the list of lemmatized words.\n",
    "print(lemmatized_words_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b53e3c",
   "metadata": {},
   "source": [
    "In Natural Language Processing (NLP), both **lemmatization** and **stemming** are techniques used to reduce words to their base or root form. This process, known as **word normalization**, helps in reducing redundancy and improving the accuracy of various NLP tasks like information retrieval, text classification, and sentiment analysis. However, they achieve this goal in different ways and with different levels of linguistic sophistication.\n",
    "\n",
    "-----\n",
    "\n",
    "### Lemmatization\n",
    "\n",
    "**Lemmatization** is a more sophisticated and linguistically informed process. It aims to reduce a word to its **lemma**, which is its canonical or dictionary form. This means that the output of lemmatization is always a valid word that can be found in a dictionary. It achieves this by considering the word's morphological analysis and its Part-of-Speech (POS) tag.\n",
    "\n",
    "For example:\n",
    "\n",
    "  * \"running\" would become \"run\"\n",
    "  * \"better\" would become \"good\" (as \"better\" is the comparative form of \"good\")\n",
    "  * \"flies\" (verb) would become \"fly\"\n",
    "  * \"flies\" (noun, plural of \"fly\") would become \"fly\"\n",
    "\n",
    "Lemmatization typically requires a lexicon (dictionary) and morphological analysis (understanding word structure and derivations). This makes it more accurate but also computationally more expensive.\n",
    "\n",
    "-----\n",
    "\n",
    "### Stemming\n",
    "\n",
    "**Stemming** is a more crude and heuristic process. It primarily involves **removing suffixes** from words to get to a \"stem\" or \"root form.\" The stem is often not a valid word itself. It works by applying a set of rules, often regular expressions, to chop off word endings.\n",
    "\n",
    "For example:\n",
    "\n",
    "  * \"running\" might become \"run\" (if the rule removes \"ning\")\n",
    "  * \"connection\" might become \"connect\" (if the rule removes \"ion\")\n",
    "  * \"flies\" might become \"fli\" (if the rule just removes \"es\")\n",
    "\n",
    "Stemming algorithms (like the Porter Stemmer or Snowball Stemmer) are typically faster and simpler to implement than lemmatization, as they don't require external lexical resources. However, their output might not always be linguistically correct, leading to \"over-stemming\" (removing too much of the word) or \"under-stemming\" (not removing enough).\n",
    "\n",
    "-----\n",
    "\n",
    "### Key Differences:\n",
    "\n",
    "| Feature           | Lemmatization                                      | Stemming                                           |\n",
    "| :---------------- | :------------------------------------------------- | :------------------------------------------------- |\n",
    "| **Output** | A **valid word** (lemma/dictionary form).           | Often a **root or \"stem\"** that may not be a real word. |\n",
    "| **Approach** | Linguistically informed; uses vocabulary and morphological analysis. | Rule-based; chops off prefixes/suffixes heuristically. |\n",
    "| **Accuracy** | Generally **more accurate** and reliable.          | Less accurate; can lead to over-stemming or under-stemming. |\n",
    "| **POS Tagging** | **Requires or benefits significantly** from POS tagging for better accuracy. | Does **not typically use** POS tagging.         |\n",
    "| **Speed/Complexity** | Slower and more computationally intensive.        | Faster and less computationally intensive.         |\n",
    "| **Language Dependency** | Highly language-dependent (needs a lexicon for each language). | Can be language-dependent, but simpler rules can be generalized. |\n",
    "| **Use Cases** | When linguistic accuracy is crucial (e.g., machine translation, complex question answering). | When speed and basic normalization are sufficient (e.g., information retrieval index, large-scale text analysis where some errors are acceptable). |\n",
    "| **Example** | \"am\", \"are\", \"is\" $\\\\rightarrow$ \"be\"\\<br\\>\"better\" $\\\\rightarrow$ \"good\"\\<br\\>\"flies\" (noun) $\\\\rightarrow$ \"fly\" | \"am\", \"are\", \"is\" $\\\\rightarrow$ \"be\" (might be different depending on stemmer)\\<br\\>\"better\" $\\\\rightarrow$ \"better\"\\<br\\>\"flies\" $\\\\rightarrow$ \"fli\" |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d789d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Felipe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Felipe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Organization', 'NN'), (':', ':'), ('VTT', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups # Import the function to fetch the 20 Newsgroups dataset.\n",
    "import nltk # Import the Natural Language Toolkit (NLTK) library for NLP tasks.\n",
    "\n",
    "# This model is used by nltk.word_tokenize to split text into words.\n",
    "nltk.download('punkt')\n",
    "\n",
    "# This model is used by nltk.pos_tag to assign grammatical tags to words.\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Fetch a sample of text from the 20 Newsgroups dataset.\n",
    "# 'subset='train'' specifies that we want to load only the training subset of the data.\n",
    "newsgroups_data = fetch_20newsgroups(subset='train')\n",
    "\n",
    "# Select a sample line from one of the documents.\n",
    "# newsgroups_data.data is a list where each element is a document (as a string).\n",
    "# [5] accesses the 6th document in the list.\n",
    "# .split(\"\\n\")[2] splits that document into lines and selects the 3rd line (index 2).\n",
    "sample_text_line = newsgroups_data.data[5].split(\"\\n\")[2]\n",
    "\n",
    "# Tokenize the selected sample text line into individual words.\n",
    "# Example: \"This is a sample sentence.\" -> ['This', 'is', 'a', 'sample', 'sentence', '.']\n",
    "word_tokens = nltk.word_tokenize(sample_text_line)\n",
    "\n",
    "# Perform Part-of-Speech (POS) tagging on the word tokens.\n",
    "# This assigns a grammatical category (e.g., noun, verb, adjective) to each word.\n",
    "# The result is a list of tuples, where each tuple is (word, pos_tag).\n",
    "# Example: [('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('sample', 'NN'), ('sentence', 'NN'), ('.', '.')]\n",
    "pos_tags = nltk.pos_tag(word_tokens)\n",
    "\n",
    "# Print the list of words with their corresponding POS tags.\n",
    "print(pos_tags)\n",
    "\n",
    "frase = 'oi, eu sou o goku'\n",
    "print(frase.split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "28caef32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['oi,', 'eu', 'sou', 'o', 'goku']\n",
      "oi, eu sou o goku\n"
     ]
    }
   ],
   "source": [
    "frase = 'oi, eu sou o goku'\n",
    "print(frase.split(' '))\n",
    "print(frase)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
